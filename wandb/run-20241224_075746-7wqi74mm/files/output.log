<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
</xml>
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz
100.0%
Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz

<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Using downloaded and verified file: ./data/MNIST/raw/train-images-idx3-ubyte.gz
Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz
100.0%
Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz
100.0%
Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Failed to download (trying next):
HTTP Error 403: Forbidden

Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz
Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz
100.0%
Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw

<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c1f670&gt;" isContainer="True" shape="1667" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c1f790&gt;" isContainer="True" shape="10000" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c1f670&gt;" isContainer="True" shape="1667" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c1f790&gt;" isContainer="True" shape="10000" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c1f670&gt;" isContainer="True" shape="1667" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c1f790&gt;" isContainer="True" shape="10000" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c314f0&gt;" isContainer="True" shape="1667" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31490&gt;" isContainer="True" shape="10000" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c314f0&gt;" isContainer="True" shape="1667" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31490&gt;" isContainer="True" shape="10000" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c314f0&gt;" isContainer="True" shape="1667" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31490&gt;" isContainer="True" shape="10000" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c314f0&gt;" isContainer="True" shape="1667" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31490&gt;" isContainer="True" shape="10000" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c314f0&gt;" isContainer="True" shape="1667" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31490&gt;" isContainer="True" shape="10000" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31250&gt;" isContainer="True" shape="1667" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1262a0610&gt;" isContainer="True" shape="10000" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31250&gt;" isContainer="True" shape="1667" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1262a0610&gt;" isContainer="True" shape="10000" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
epoch 0 of 10: training_loss = 3033.8235386097367, training_accuracy = 0.9047166666666666, test_loss = 309.0865886576921, test_accuracy = 0.9429
epoch 1 of 10: training_loss = 1721.5165286411357, training_accuracy = 0.9479, test_loss = 217.0744534841001, test_accuracy = 0.9601
epoch 2 of 10: training_loss = 1425.569621132432, training_accuracy = 0.9573666666666667, test_loss = 220.73436139735577, test_accuracy = 0.9616
epoch 3 of 10: training_loss = 1298.0433552298464, training_accuracy = 0.9617, test_loss = 287.3640399043231, test_accuracy = 0.949
epoch 4 of 10: training_loss = 1196.879600705483, training_accuracy = 0.96515, test_loss = 238.18068510148763, test_accuracy = 0.962
epoch 5 of 10: training_loss = 1145.613586347353, training_accuracy = 0.9668166666666667, test_loss = 273.6184614143973, test_accuracy = 0.9555
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="6" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9555" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31250&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="273.6184614143973" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1262a0610&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9668166666666667" />
<var name="training_loss" type="float" qualifier="builtins" value="1145.613586347353" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9555" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31250&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="273.6184614143973" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1262a0610&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9668166666666667" />
<var name="training_loss" type="float" qualifier="builtins" value="1145.613586347353" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
epoch 1 of 10: training_loss = 1074.2506, training_accuracy = 0.9693, test_loss = 239.1032, test_accuracy = 0.9643
epoch 2 of 10: training_loss = 1007.0872, training_accuracy = 0.9718, test_loss = 221.7838, test_accuracy = 0.9680
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="2" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.968" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31250&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="221.78380072785657" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1262a0610&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9717833333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="1007.0871659185926" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="2" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.968" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31250&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="221.78380072785657" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1262a0610&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9717833333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="1007.0871659185926" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
loss = 0.0002688590029720217
loss = 1.4106373100730707e-06
loss = 0.0007808294612914324
loss = 0.006298270542174578
loss = 0.0005486204172484577
loss = 0.001581773511134088
loss = 0.15850073099136353
loss = 0.22024135291576385
loss = 0.040559086948633194
loss = 0.00020429644791875035
loss = 6.710667366860434e-05
loss = 0.00013911986025050282
loss = 7.59278264013119e-05
loss = 0.0036281694192439318
loss = 0.00999766681343317
loss = 5.661528848577291e-05
loss = 0.004523166920989752
loss = 0.00434098718687892
loss = 4.76834065921139e-06
loss = 0.0005057314992882311
loss = 0.0
loss = 5.630141822621226e-05
loss = 3.1590186608809745e-06
loss = 0.00031184832914732397
loss = 0.023574860766530037
loss = 0.0026164462324231863
loss = 0.00010487461258890107
loss = 0.1385526806116104
loss = 5.4356514738174155e-05
loss = 6.2187114053813275e-06
loss = 0.00134439452085644
loss = 1.3688763829122763e-05
loss = 0.05096231400966644
loss = 0.006690185982733965
loss = 0.003955121152102947
loss = 0.016086434945464134
loss = 2.6502826585783623e-05
loss = 0.1445690542459488
loss = 0.00010194634523941204
loss = 0.00042902131099253893
loss = 0.007141928654164076
loss = 0.00026816295576281846
loss = 4.748460014525335e-06
loss = 0.024990389123558998
loss = 0.025289734825491905
loss = 2.8610020308406092e-06
loss = 0.0067667788825929165
loss = 0.0017428280552849174
loss = 0.9654704928398132
loss = 0.022541353479027748
loss = 9.059786862053443e-06
loss = 0.0018221883801743388
loss = 3.973642748178463e-08
loss = 0.00047834819997660816
loss = 0.015982335433363914
loss = 0.00749213295057416
loss = 0.030268391594290733
loss = 3.4429827792337164e-05
loss = 8.265083124570083e-06
loss = 8.007291035028175e-05
loss = 0.00014487294538412243
loss = 0.04341799393296242
loss = 0.429788738489151
loss = 1.1920926823449918e-07
loss = 6.110734102549031e-05
loss = 0.04742077365517616
loss = 8.809284190647304e-05
loss = 0.0767214372754097
loss = 0.12433402985334396
loss = 0.037883076816797256
loss = 0.010342906229197979
loss = 0.9730921387672424
loss = 0.008403516374528408
loss = 0.0012139725731685758
loss = 9.005866741063073e-05
loss = 0.0011780844070017338
loss = 0.012359113432466984
loss = 0.06028112396597862
loss = 0.0018983852351084352
loss = 0.0016846562502905726
loss = 2.356206823606044e-05
loss = 0.011256593279540539
loss = 0.00039887416642159224
loss = 2.6027230433101067e-06
loss = 0.01426513958722353
loss = 0.0035943707916885614
loss = 1.3331049558473751e-05
loss = 0.012198646552860737
loss = 0.23850780725479126
loss = 0.0005449550808407366
loss = 1.0019924640655518
loss = 0.009208734147250652
loss = 0.006731617730110884
loss = 0.0043106116354465485
loss = 0.01400193851441145
loss = 0.006461579818278551
loss = 0.08664584904909134
loss = 0.026819990947842598
loss = 0.00030145529308356345
loss = 0.0008563640876673162
loss = 1.9688237443915568e-05
loss = 1.382794198434567e-05
loss = 0.0004524615651462227
loss = 1.076839180313982e-05
loss = 6.0389520513126627e-05
loss = 1.9868213740892315e-08
loss = 0.022664813324809074
loss = 0.20563741028308868
loss = 0.6759994626045227
loss = 0.008800078183412552
loss = 0.0008129308116622269
loss = 0.09639843553304672
loss = 0.2925931513309479
loss = 7.242832361953333e-05
loss = 0.0026007776614278555
loss = 0.0002204311895184219
loss = 0.005181949120014906
loss = 0.25957468152046204
loss = 0.004242480266839266
loss = 0.17723804712295532
loss = 0.00015667859406676143
loss = 0.0003754585050046444
loss = 0.006094321142882109
loss = 0.3723800480365753
loss = 0.01646151766180992
loss = 0.004101024009287357
loss = 0.0267839003354311
loss = 4.3307383748469874e-05
loss = 0.016155395656824112
loss = 0.22930912673473358
loss = 0.0002098903787555173
loss = 0.0001160728934337385
loss = 6.24937383690849e-05
loss = 1.2000057722616475e-05
loss = 7.688823643547948e-06
loss = 0.09554782509803772
loss = 0.01660618931055069
loss = 0.01878664456307888
loss = 0.0844012200832367
loss = 0.00022309558698907495
loss = 5.086185865366133e-06
loss = 0.00015311357856262475
loss = 0.0006352203781716526
loss = 3.770784314838238e-05
loss = 4.947148227074649e-06
loss = 0.00018951062520500273
loss = 0.23694248497486115
loss = 8.670874376548454e-05
loss = 0.009587139822542667
loss = 1.6947205949691124e-05
loss = 0.28534063696861267
loss = 9.825910092331469e-05
loss = 0.0006041420274414122
loss = 0.1756213754415512
loss = 4.9073782975028735e-06
loss = 0.011810938827693462
loss = 0.00017119600670412183
loss = 0.0008640969172120094
loss = 0.00012633588630706072
loss = 1.9868208767093165e-07
loss = 0.0
loss = 0.015047225169837475
loss = 0.00015294461627490819
loss = 1.1026596439478453e-05
loss = 8.002363028936088e-05
loss = 0.015841657295823097
loss = 0.0001794311829144135
loss = 0.06129303574562073
loss = 0.00011940608237637207
loss = 1.9868213740892315e-08
loss = 2.6623195026331814e-06
loss = 0.48302599787712097
loss = 0.0008560869027860463
loss = 1.0410731192678213e-05
loss = 8.08617642178433e-06
loss = 6.556501261911762e-07
loss = 6.79048098390922e-05
loss = 0.01389854121953249
loss = 3.884040052071214e-05
loss = 2.0026367565151304e-05
loss = 1.641035487409681e-05
loss = 0.00019025035726372153
loss = 3.874271442327881e-06
loss = 0.0002221450413344428
loss = 0.19169054925441742
loss = 0.0012045052135363221
loss = 0.0016890928382053971
loss = 0.006273871753364801
loss = 0.05108495056629181
loss = 0.004745073150843382
loss = 0.00045186138595454395
loss = 1.0132774832527502e-06
loss = 0.002642316510900855
loss = 0.0022526432294398546
loss = 4.351084044174058e-06
loss = 2.2887528757564723e-05
loss = 0.1702955812215805
loss = 0.0011154888197779655
loss = 0.0054147131741046906
loss = 0.001350034144707024
loss = 0.005977581720799208
loss = 0.11018245667219162
loss = 0.07046113163232803
loss = 0.0009836637182161212
loss = 1.649057594477199e-06
loss = 0.0002002524706767872
loss = 3.3179587717313552e-06
loss = 0.32500436902046204
loss = 0.0015564505010843277
loss = 0.00022722496942151338
loss = 0.000899497710634023
loss = 7.371059837169014e-06
loss = 1.3013424904784188e-05
loss = 0.0003980428446084261
loss = 0.6766422390937805
loss = 1.9868213740892315e-08
loss = 1.1920926112907182e-07
loss = 1.0530121699048323e-06
loss = 0.029658779501914978
loss = 0.00346975470893085
loss = 0.0001476287579862401
loss = 2.993907401105389e-05
loss = 0.3907613456249237
loss = 1.6093181329779327e-06
loss = 0.0006701780366711318
loss = 0.0004193488275632262
loss = 0.03127828240394592
loss = 0.04956302046775818
loss = 0.12161916494369507
loss = 4.688855369749945e-06
loss = 0.00046073063276708126
loss = 0.007026634644716978
loss = 8.49548086989671e-05
loss = 0.0018258867785334587
loss = 2.153622335754335e-05
loss = 9.543439227854833e-05
loss = 0.210029736161232
loss = 0.0001436449820175767
loss = 0.0013163272524252534
loss = 0.0006554020219482481
loss = 0.0026179365813732147
loss = 4.102287857676856e-05
loss = 0.006609535310417414
loss = 0.021501628682017326
loss = 9.020048310048878e-06
loss = 1.331167368334718e-06
loss = 0.0001678800181252882
loss = 0.01180245727300644
loss = 0.0005451071774587035
loss = 0.22228799760341644
loss = 0.000826766190584749
loss = 0.0009062227909453213
loss = 0.0006753621273674071
loss = 0.001094591454602778
loss = 0.0006048360955901444
loss = 0.006942104548215866
loss = 0.07726400345563889
loss = 4.371005104530923e-07
loss = 7.510019713663496e-06
loss = 0.019230952486395836
loss = 0.0014438816579058766
loss = 0.0074563417583703995
loss = 0.011930494569242
loss = 0.017586058005690575
loss = 2.5431238555029267e-06
loss = 7.947276685627003e-07
loss = 7.312033994821832e-05
loss = 0.0011131089413538575
loss = 0.053837697952985764
loss = 0.0001438145263819024
loss = 9.397470421390608e-06
loss = 0.002703560283407569
loss = 0.5591573119163513
loss = 0.004561828915029764
loss = 0.0004585595161188394
loss = 0.06077442690730095
loss = 0.0012174074072390795
loss = 0.004630043171346188
loss = 5.9604641222676946e-08
loss = 0.16280624270439148
loss = 0.2953997254371643
loss = 0.041671887040138245
loss = 0.03976733610033989
loss = 0.017825284972786903
loss = 0.0017953976057469845
loss = 0.080239437520504
loss = 0.001576248207129538
loss = 0.03143620118498802
loss = 0.10443264991044998
loss = 0.01663425751030445
loss = 0.00043087638914585114
loss = 2.0820712961722165e-05
loss = 0.0019667972810566425
loss = 0.00017892861797008663
loss = 0.0006540885660797358
loss = 0.013289122842252254
loss = 0.00014964841830078512
loss = 0.00012891592632513493
loss = 0.6875081658363342
loss = 7.206645386759192e-05
loss = 4.825297583010979e-05
loss = 0.006293769925832748
loss = 0.000527381373103708
loss = 0.0001467832480557263
loss = 1.3589394256996457e-05
loss = 0.0020390439312905073
loss = 1.3621267080307007
loss = 0.12521424889564514
loss = 6.0796028265031055e-06
loss = 0.0124375494197011
loss = 0.0018730153096839786
loss = 0.008149992674589157
loss = 1.5695455658715218e-05
loss = 0.05737413838505745
loss = 0.0005410535959526896
loss = 0.804757833480835
loss = 7.790492963977158e-05
loss = 0.0005769099807366729
loss = 0.026017969474196434
loss = 0.00021678616758435965
loss = 0.009394855238497257
loss = 1.7076395750045776
loss = 8.404095751757268e-06
loss = 0.00010419698810437694
loss = 0.004966260865330696
loss = 3.94536618841812e-05
loss = 3.0795492875768105e-06
loss = 0.013087446801364422
loss = 0.004040878731757402
loss = 0.0
loss = 4.738236384582706e-05
loss = 0.26283255219459534
loss = 0.004214293789118528
loss = 1.0867559467442334e-05
loss = 0.013271640054881573
loss = 4.768367034557741e-07
loss = 0.006124102044850588
loss = 0.002785689430311322
loss = 0.0954204574227333
loss = 5.761773422818806e-07
loss = 0.033858612179756165
loss = 3.5103661502944306e-05
loss = 1.982758658414241e-05
loss = 0.0003274527844041586
loss = 0.005619628354907036
loss = 0.19153685867786407
loss = 0.27591070532798767
loss = 0.0001425332884537056
loss = 0.07083998620510101
loss = 0.00010129730799235404
loss = 0.27296850085258484
loss = 5.761773422818806e-07
loss = 0.6976413726806641
loss = 5.960463411724959e-08
loss = 3.973642748178463e-08
loss = 0.04968715086579323
loss = 2.26090141950408e-05
loss = 6.879429565742612e-05
loss = 0.0019550505094230175
loss = 3.4509146644268185e-05
loss = 6.4968285187205765e-06
loss = 3.377593884579255e-07
loss = 0.2232438176870346
loss = 0.9488877654075623
loss = 0.8888158202171326
loss = 0.0006878476706333458
loss = 0.00023375290038529783
loss = 0.010879267007112503
loss = 0.0
loss = 0.02057866007089615
loss = 1.3907744289554103e-07
loss = 0.03606139123439789
loss = 0.06733240187168121
loss = 0.00014648390060756356
loss = 0.0002491902851033956
loss = 0.7055894732475281
loss = 0.0026951951440423727
loss = 0.5212584733963013
loss = 0.3874761760234833
loss = 0.038728948682546616
loss = 0.001045513665303588
loss = 0.35489580035209656
loss = 4.32485066994559e-05
loss = 0.8855993747711182
loss = 0.0026104289572685957
loss = 2.2032729248167016e-05
loss = 9.283819235861301e-05
loss = 0.008507171645760536
loss = 1.3768108146905433e-05
loss = 1.4086143892200198e-05
loss = 0.0002356034383410588
loss = 3.220383950974792e-05
loss = 0.002625059336423874
loss = 0.06831511110067368
loss = 0.001135994796641171
loss = 2.896655678341631e-05
loss = 0.34132349491119385
loss = 0.11897885799407959
loss = 0.3296966254711151
loss = 2.042360392806586e-05
loss = 0.006148198619484901
loss = 0.003218613564968109
loss = 3.812274371739477e-05
loss = 0.000477894238429144
loss = 4.3607713450910524e-05
loss = 0.7837164402008057
loss = 3.0994226563052507e-06
loss = 0.03724613040685654
loss = 0.515653669834137
loss = 0.00428475858643651
loss = 0.0010279766283929348
loss = 0.0023436055053025484
loss = 0.006587215233594179
loss = 0.004088831599801779
loss = 3.3775930319279723e-07
loss = 1.0420401096343994
loss = 0.000120318349217996
loss = 0.12084656208753586
loss = 0.0005091847269795835
loss = 0.44583895802497864
loss = 5.437049549072981e-05
loss = 0.013394718058407307
loss = 0.15388944745063782
loss = 3.973642392907095e-08
loss = 0.0003063988115172833
loss = 0.000289990653982386
loss = 0.312632292509079
loss = 3.025773366971407e-05
loss = 0.019109254702925682
loss = 1.690700628387276e-05
loss = 3.774956951474451e-07
loss = 0.00456186244264245
loss = 8.920726941141766e-06
loss = 0.000855232763569802
loss = 0.002423120429739356
loss = 1.1523534340085462e-06
loss = 0.0025008125230669975
loss = 0.021879004314541817
loss = 3.695473651532666e-06
loss = 0.2880151867866516
loss = 0.0005239550373516977
loss = 0.0019646543078124523
loss = 0.029453447088599205
loss = 5.324605353962397e-06
loss = 0.0002644896158017218
loss = 0.000732117856387049
loss = 5.9604641222676946e-08
loss = 0.00011770497803809121
loss = 0.0025258075911551714
loss = 0.823256254196167
loss = 0.0011945451842620969
loss = 9.019943718158174e-06
loss = 0.011006648652255535
loss = 0.15819372236728668
loss = 0.07339918613433838
loss = 8.423927283729427e-06
loss = 0.7136560082435608
loss = 0.11107876151800156
loss = 0.0064626955427229404
loss = 2.68219196186692e-06
loss = 0.000566079281270504
loss = 0.5117239952087402
loss = 0.013085789047181606
loss = 1.5138906746869907e-05
loss = 0.0007735313265584409
loss = 0.004174637142568827
loss = 4.783811164088547e-05
loss = 0.14742478728294373
loss = 0.005699308589100838
loss = 0.002009589457884431
loss = 1.2039728062518407e-05
loss = 3.635713073890656e-05
loss = 0.0009815614903345704
loss = 1.2417476682458073e-05
loss = 0.0033693474251776934
loss = 0.002399836666882038
loss = 0.0004437130701262504
loss = 0.6503705382347107
loss = 0.00010232964996248484
loss = 0.8710330128669739
loss = 0.012828814797103405
loss = 0.0728403776884079
loss = 0.00017183797899633646
loss = 0.06810265779495239
loss = 0.015534843318164349
loss = 2.775405846477952e-05
loss = 0.0010143083054572344
loss = 0.25292548537254333
loss = 0.05650222674012184
loss = 3.377593884579255e-07
loss = 0.6107291579246521
loss = 0.30428752303123474
loss = 0.09616124629974365
loss = 2.7257889087195508e-05
loss = 0.0005707804812118411
loss = 0.19848178327083588
loss = 0.0016519733471795917
loss = 0.4610321819782257
loss = 0.024620993062853813
loss = 0.023798761889338493
loss = 0.0009734486229717731
loss = 0.0026448110584169626
loss = 5.625822814181447e-05
loss = 0.327670156955719
loss = 2.060235601675231e-05
loss = 6.556506946253648e-07
loss = 0.00015678659838158637
loss = 1.567565050208941e-05
loss = 0.007189339492470026
loss = 0.0008492188062518835
loss = 0.00045194270205684006
loss = 0.08244749158620834
loss = 0.46621373295783997
loss = 0.022046929225325584
loss = 9.434800449525937e-05
loss = 0.00021150243992451578
loss = 0.0014431305462494493
loss = 0.17376594245433807
loss = 0.000928580469917506
loss = 0.00343970931135118
loss = 3.019944188054069e-06
loss = 0.0007629182655364275
loss = 0.0004129906592424959
loss = 0.6943873763084412
loss = 6.124270294094458e-05
loss = 6.519734597532079e-05
loss = 0.3720267713069916
loss = 0.040743637830019
loss = 1.0132778243132634e-06
loss = 0.02455136924982071
loss = 1.0249055624008179
loss = 0.026706362143158913
loss = 0.0829073041677475
loss = 0.0004897579201497138
loss = 0.0001564870326546952
loss = 0.01820613630115986
loss = 0.00047516878112219274
loss = 0.00026232973323203623
loss = 0.0035859092604368925
loss = 0.0004990503657609224
loss = 0.0010065034730359912
loss = 0.0006654983735643327
loss = 0.0009457692503929138
loss = 0.00036382197868078947
loss = 5.9604641222676946e-08
loss = 0.0023131088819354773
loss = 0.000158689494128339
loss = 0.11412198096513748
loss = 0.049362245947122574
loss = 0.00017830201250035316
loss = 0.0033546630293130875
loss = 0.0001931437145685777
loss = 0.1219739243388176
loss = 0.46932467818260193
loss = 0.0005262787453830242
loss = 0.016238270327448845
loss = 0.00017192092491313815
loss = 7.1056725573726e-05
loss = 0.00016663195856381208
loss = 0.0003477850405033678
loss = 0.0007943985401652753
loss = 0.007712574675679207
loss = 0.0008955546072684228
loss = 0.037369728088378906
loss = 0.014605071395635605
loss = 1.2894274732389022e-05
loss = 0.005505360197275877
loss = 0.004030146636068821
loss = 2.9860588256269693e-05
loss = 0.00012174793664598837
loss = 0.0028493383433669806
loss = 0.020512321963906288
loss = 0.0011137056862935424
loss = 0.16650976240634918
loss = 0.005703577771782875
loss = 6.619231862714514e-05
loss = 0.00023318907187785953
loss = 0.7069985866546631
loss = 0.0035559777170419693
loss = 0.4707132577896118
loss = 0.1330188363790512
loss = 0.14048932492733002
loss = 3.7749575199086394e-07
loss = 0.3847828805446625
loss = 1.8580290079116821
loss = 0.19313867390155792
loss = 0.0008917823433876038
loss = 0.003945467062294483
loss = 3.5264765756437555e-05
loss = 0.15321360528469086
loss = 0.00024683246738277376
loss = 9.7949314294965e-06
loss = 0.14946381747722626
loss = 0.00016228057211264968
loss = 0.0012274679029360414
loss = 1.0023337602615356
loss = 0.0006683100364170969
loss = 0.006510743405669928
loss = 0.11425129324197769
loss = 0.007447489071637392
loss = 0.0011019388912245631
loss = 0.001197047415189445
loss = 4.291514869692037e-06
loss = 0.04309278726577759
loss = 0.009956193156540394
loss = 0.021728264167904854
loss = 0.006391261238604784
loss = 0.057179491966962814
loss = 0.00011734768486348912
loss = 0.009242824278771877
loss = 0.00010436963202664629
loss = 0.90469890832901
loss = 0.00039511278737336397
loss = 0.043705377727746964
loss = 7.205167639767751e-05
loss = 0.012981751002371311
loss = 0.0009534449782222509
loss = 0.0016203889390453696
loss = 0.0014471417525783181
loss = 0.0001874533627415076
loss = 0.007374059874564409
loss = 0.00036309464485384524
loss = 0.018510006368160248
loss = 0.0002836759085766971
loss = 0.0010773086687549949
loss = 0.24449919164180756
loss = 0.002618286060169339
loss = 0.037315625697374344
loss = 8.443826118309516e-06
loss = 0.00225316290743649
loss = 0.07217953354120255
loss = 0.0028592164162546396
loss = 0.004550624638795853
loss = 1.4264532327651978
loss = 0.5809954404830933
loss = 0.6002634167671204
loss = 0.00014115736121311784
loss = 0.000177627764060162
loss = 0.0016074379673227668
loss = 0.0005612110835500062
loss = 0.0714530423283577
loss = 0.053285177797079086
loss = 0.14048682153224945
loss = 0.16106124222278595
loss = 0.0005442998372018337
loss = 0.16473603248596191
loss = 0.00026014019385911524
loss = 0.02783448062837124
loss = 0.01394468080252409
loss = 0.00014012621250003576
loss = 0.07381992042064667
loss = 0.036980509757995605
loss = 9.395580127602443e-05
loss = 0.00015257523045875132
loss = 0.0018868468469008803
loss = 0.033667534589767456
loss = 0.007535045966506004
loss = 0.031901270151138306
loss = 0.13581684231758118
loss = 0.028576917946338654
loss = 3.4173106087109772e-06
loss = 0.2784382402896881
loss = 0.063709557056427
loss = 0.0010976694757118821
loss = 1.0808126717165578e-05
loss = 0.0064619979821145535
loss = 0.0002118453267030418
loss = 0.0035227180924266577
loss = 0.00012057107960572466
loss = 0.0058358535170555115
loss = 0.0006261938833631575
loss = 0.0007278979173861444
loss = 0.05865607038140297
loss = 0.0001758420985424891
loss = 0.0001761796447681263
loss = 0.00012663297820836306
loss = 1.3809409141540527
loss = 0.010819588787853718
loss = 0.020911065861582756
loss = 0.020794255658984184
loss = 0.06795511394739151
loss = 0.004809082020074129
loss = 0.05557731166481972
loss = 0.0060684517957270145
loss = 0.0017532430356368423
loss = 1.6390638847951777e-05
loss = 0.0031655586790293455
loss = 0.018981335684657097
loss = 0.049493517726659775
loss = 0.004126576241105795
loss = 0.03858526423573494
loss = 0.10499034076929092
loss = 0.040975410491228104
loss = 0.0008999179117381573
loss = 3.5481607483234257e-05
loss = 0.02010398544371128
loss = 0.006891101598739624
loss = 0.2891494929790497
loss = 0.02800564467906952
loss = 0.0068829674273729324
loss = 2.026496258622501e-05
loss = 0.002119851764291525
loss = 0.0004889743286184967
loss = 0.026967322453856468
loss = 0.000587365182582289
loss = 0.007110824342817068
loss = 5.5149761465145275e-05
loss = 1.585418613103684e-05
loss = 0.005705090239644051
loss = 0.0026790390256792307
loss = 0.00029055291088297963
loss = 0.004970203619450331
loss = 0.07239902764558792
loss = 0.007234388962388039
loss = 0.000144857433042489
loss = 0.00012546560901682824
loss = 2.47743464569794e-05
loss = 0.41409340500831604
loss = 0.0007931184954941273
loss = 0.0039632548578083515
loss = 0.14582790434360504
loss = 0.0013935441384091973
loss = 0.01949811540544033
loss = 0.0033914390951395035
loss = 0.18289689719676971
loss = 1.4389740228652954
loss = 0.001400345005095005
loss = 0.0064824591390788555
loss = 0.002391320187598467
loss = 9.159240289591253e-05
loss = 0.34806978702545166
loss = 0.0015383813297376037
loss = 1.3113002523823525e-06
loss = 0.01839105598628521
loss = 0.2764984369277954
loss = 0.0037032684776932
loss = 0.08946245163679123
loss = 0.0001188811074825935
loss = 0.015408955514431
loss = 0.017172588035464287
loss = 0.00023356465680990368
loss = 0.010204837657511234
loss = 0.197073295712471
loss = 0.034178152680397034
loss = 0.0284894909709692
loss = 0.6335418820381165
loss = 9.817321551963687e-05
loss = 0.6347994208335876
loss = 0.0032623065635561943
loss = 0.20875205099582672
loss = 0.014956191182136536
loss = 0.0009178384207189083
loss = 0.09155365079641342
loss = 0.013300665654242039
loss = 0.005227701738476753
loss = 0.001739741419441998
loss = 0.0005010170280002058
loss = 0.0017350591951981187
loss = 0.0260759424418211
loss = 0.3711768388748169
loss = 0.0017482646508142352
loss = 0.008107470348477364
loss = 0.011774993501603603
loss = 0.0002455491339787841
loss = 7.917927723610774e-05
loss = 1.1511908769607544
loss = 0.004123298451304436
loss = 0.0561458058655262
loss = 4.768367887209024e-07
loss = 0.002046654699370265
loss = 2.2131722289486788e-05
loss = 0.7475314736366272
loss = 0.09203975647687912
loss = 0.04688991233706474
loss = 1.2246953248977661
loss = 0.6699898838996887
loss = 1.2039942703268025e-05
loss = 0.01221857126802206
loss = 0.061034638434648514
loss = 0.00808078795671463
loss = 0.00021521258167922497
loss = 2.097644090652466
loss = 0.21354402601718903
loss = 0.012546462006866932
loss = 4.569683085264842e-07
loss = 8.106081622827332e-06
loss = 0.09110298752784729
loss = 0.010163852013647556
loss = 0.021110378205776215
loss = 0.049923140555620193
loss = 0.0015577888116240501
loss = 1.0907472642429639e-05
loss = 0.03383251652121544
loss = 0.5836402177810669
loss = 0.0016792205860838294
loss = 0.025432666763663292
loss = 0.02315368317067623
loss = 0.18001534044742584
loss = 0.06425970792770386
loss = 0.5307495594024658
loss = 3.319650568300858e-05
loss = 0.8210940957069397
loss = 0.0002935196680482477
loss = 8.44385886011878e-06
loss = 0.09795290976762772
loss = 0.006841694470494986
loss = 0.2872053384780884
loss = 0.002315278397873044
loss = 0.001616822904907167
loss = 0.2058640569448471
loss = 0.0002942533465102315
loss = 0.01627429760992527
loss = 5.808999776490964e-05
loss = 5.622671324090334e-06
loss = 0.058607909828424454
loss = 0.023703863844275475
loss = 0.24199575185775757
loss = 0.4985884726047516
loss = 0.07756263762712479
loss = 1.5375499725341797
loss = 6.576311079697916e-06
loss = 0.006400690879672766
loss = 0.3899388611316681
loss = 0.0007780101732350886
loss = 1.2476774827518966e-05
loss = 6.000139819661854e-06
loss = 0.49699094891548157
loss = 0.0013955002650618553
loss = 0.0013036582386121154
loss = 0.011588971130549908
loss = 0.2677852511405945
loss = 4.390817593957763e-06
loss = 0.0012531105894595385
loss = 0.0011705479118973017
loss = 0.019614985212683678
loss = 0.6294174790382385
loss = 0.0003681817033793777
loss = 7.06613645888865e-05
loss = 0.015688074752688408
loss = 4.967046152160037e-07
loss = 0.03895658627152443
loss = 4.986847216059687e-06
loss = 0.0674237534403801
loss = 0.15497854351997375
loss = 0.0001993802870856598
loss = 0.0001098659195122309
loss = 0.00015395994705613703
loss = 0.00024926490732468665
loss = 0.0025467523373663425
loss = 0.011441126465797424
loss = 0.003237158292904496
loss = 0.050708919763565063
loss = 1.008278727531433
loss = 0.012698945589363575
loss = 0.0003125644288957119
loss = 1.0124739408493042
loss = 0.909343957901001
loss = 0.008603998459875584
loss = 1.3677068948745728
loss = 7.352207467192784e-05
loss = 0.0011926263105124235
loss = 0.18212640285491943
loss = 0.0045402985997498035
loss = 0.003870287211611867
loss = 0.00030035892268642783
loss = 0.0025751753710210323
loss = 0.05709030106663704
loss = 0.004541635047644377
loss = 0.04889361932873726
loss = 0.11815307289361954
loss = 0.003290581749752164
loss = 4.445964805199765e-05
loss = 0.05228513106703758
loss = 0.11167225241661072
loss = 0.03489921987056732
loss = 0.06515489518642426
loss = 0.007136160042136908
loss = 0.0013272179057821631
loss = 1.4305077229437302e-06
loss = 0.07662736624479294
loss = 0.000260011525824666
loss = 0.00013830955140292645
loss = 0.00199702731333673
loss = 2.082097671518568e-05
loss = 0.003113668644800782
loss = 0.30259039998054504
loss = 0.05872262641787529
loss = 0.00018248934065923095
loss = 8.589972276240587e-05
loss = 0.01637808419764042
loss = 0.02428244613111019
loss = 4.6248551370808855e-05
loss = 0.015453082509338856
loss = 4.19215029978659e-06
loss = 0.054764747619628906
loss = 0.019537175074219704
loss = 0.002768113510683179
loss = 0.009112093597650528
loss = 0.29065749049186707
loss = 0.0003854945825878531
loss = 0.25461044907569885
loss = 0.017618153244256973
loss = 0.056656599044799805
loss = 0.0787486582994461
loss = 0.11720141023397446
loss = 0.005093896761536598
loss = 0.33033081889152527
loss = 0.21284212172031403
loss = 0.03609316796064377
loss = 0.0013798316940665245
loss = 0.01100198458880186
loss = 0.0015889820642769337
loss = 0.5998815894126892
loss = 0.1343521922826767
loss = 0.0093580586835742
loss = 0.007615229114890099
loss = 0.0769956186413765
loss = 0.004844203125685453
loss = 0.04470120742917061
loss = 0.022231653332710266
loss = 0.00025117991026490927
loss = 0.0010223608696833253
loss = 1.6887155652511865e-05
loss = 4.402572812978178e-05
loss = 0.0008194306865334511
loss = 0.00014896877110004425
loss = 0.8737220764160156
loss = 1.786056782293599e-05
loss = 0.8722209334373474
loss = 2.3643035547138425e-06
loss = 1.0053161531686783e-05
loss = 0.00035431297146715224
loss = 0.14164017140865326
loss = 9.418951231054962e-05
loss = 0.05343717709183693
loss = 0.001638248679228127
loss = 0.0021298720967024565
loss = 0.004383503459393978
loss = 0.010793406516313553
loss = 5.9604641222676946e-08
loss = 0.011872030794620514
loss = 0.0003968224336858839
loss = 0.0010149544104933739
loss = 8.662450454721693e-06
loss = 0.0008673642878420651
loss = 0.00012627495743799955
loss = 0.010116475634276867
loss = 0.00019870320102199912
loss = 0.007586613763123751
loss = 0.0002543703594710678
loss = 0.050509121268987656
loss = 0.03375091776251793
loss = 0.09346986562013626
loss = 0.015427954494953156
loss = 0.03351946920156479
loss = 0.0009674314060248435
loss = 3.0336428608279675e-05
loss = 0.041608553379774094
loss = 0.020828619599342346
loss = 0.0002624895132612437
loss = 0.5945430397987366
loss = 3.25409637298435e-05
loss = 0.0031652965117245913
loss = 0.00013743968156632036
loss = 6.63585569782299e-06
loss = 0.13379250466823578
loss = 0.013033735565841198
loss = 0.0005011294269934297
loss = 0.05658653378486633
loss = 0.025762133300304413
loss = 8.066362170211505e-06
loss = 0.009833629243075848
loss = 0.00012329667515587062
loss = 0.0400526188313961
loss = 2.940483000202221e-06
loss = 0.00016032678831834346
loss = 0.010315909050405025
loss = 0.13424430787563324
loss = 0.018877940252423286
loss = 7.663397991564125e-05
loss = 4.569642896967707e-06
loss = 0.0005100609851069748
loss = 0.0011975945672020316
loss = 0.0009622345096431673
loss = 5.204824628890492e-05
loss = 0.0008152854279614985
loss = 0.01259998232126236
loss = 0.002420714357867837
loss = 0.014243465848267078
loss = 0.0029878292698413134
loss = 0.3429923951625824
loss = 6.58737844787538e-05
loss = 0.010970818810164928
loss = 0.0005040321848355234
loss = 3.973642748178463e-08
loss = 0.013880948536098003
loss = 0.03693549335002899
loss = 0.001289083738811314
loss = 0.6078047156333923
loss = 0.00029276893474161625
loss = 1.3609173038275912e-05
loss = 0.0025313538499176502
loss = 0.16537903249263763
loss = 2.5828671823546756e-07
loss = 0.0015903081512078643
loss = 0.004315020050853491
loss = 0.0023957269731909037
loss = 0.0033310239668935537
loss = 0.010388702154159546
loss = 0.11902590841054916
loss = 0.9142727851867676
loss = 6.3578204390069e-07
loss = 0.000757243949919939
loss = 0.1978973150253296
loss = 0.0021500233560800552
loss = 0.008572750724852085
loss = 1.2814786714443471e-05
loss = 0.00020051161118317395
loss = 0.0032662402372807264
loss = 0.02171538583934307
loss = 2.4437847514491295e-06
loss = 0.007559095975011587
loss = 7.073599408613518e-05
loss = 0.3271412253379822
loss = 0.003593583358451724
loss = 0.0012646015966311097
loss = 0.3593634068965912
loss = 0.2678755521774292
loss = 4.9471350394014735e-06
loss = 0.4842347204685211
loss = 0.055130764842033386
loss = 0.00011543402069946751
loss = 0.0006357182865031064
loss = 0.0002348460111534223
loss = 0.0028533258009701967
loss = 8.781625183473807e-06
loss = 1.2933877769683022e-05
loss = 7.52988989916048e-06
loss = 1.1403964890632778e-05
loss = 0.0009821276180446148
loss = 0.0019883441273123026
loss = 7.947276685627003e-07
loss = 0.008991556242108345
loss = 0.07089866697788239
loss = 0.22970294952392578
loss = 0.0036554113030433655
loss = 0.0012848257320001721
loss = 0.00046801366261206567
loss = 0.01236769463866949
loss = 4.980539597454481e-05
loss = 0.00017197067791130394
loss = 3.2362288038711995e-05
loss = 0.004079923965036869
loss = 0.0023043418768793344
loss = 6.357817596835957e-07
loss = 0.1692657619714737
loss = 3.645442484412342e-05
loss = 0.0001824256032705307
loss = 0.21744348108768463
loss = 0.0001568870502524078
loss = 0.0016687073512002826
loss = 0.2757887542247772
loss = 1.5298459175028256e-06
loss = 0.005523998290300369
loss = 1.1682102922350168e-05
loss = 5.145788236404769e-06
loss = 5.543141469388502e-06
loss = 0.0002446764556225389
loss = 0.09853443503379822
loss = 0.012293453328311443
loss = 0.00029830841231159866
loss = 2.6065687052323483e-05
loss = 0.00021236484462860972
loss = 0.003288632957264781
loss = 1.5317564010620117
loss = 0.5521095395088196
loss = 0.008488898165524006
loss = 5.62264904147014e-06
loss = 0.2366437166929245
loss = 7.37790614948608e-05
loss = 0.05598213151097298
loss = 0.0005321928765624762
loss = 0.02104245126247406
loss = 0.018396949395537376
loss = 0.044498588889837265
loss = 0.0001991792960325256
loss = 0.0028209055308252573
loss = 0.004659006837755442
loss = 0.0005669695674441755
loss = 4.768365045038081e-07
loss = 0.36675262451171875
loss = 0.0003279398661106825
loss = 0.9294372200965881
loss = 0.012623787857592106
loss = 0.0001940802321769297
loss = 0.0005875198403373361
loss = 0.0004515836189966649
loss = 2.6104849894181825e-05
loss = 1.7086598518289975e-06
loss = 0.006487836595624685
loss = 0.016833150759339333
loss = 0.002080390928313136
loss = 0.002933204174041748
loss = 8.443785191047937e-06
loss = 0.06483536213636398
loss = 1.7781118003767915e-05
loss = 0.0015194375300779939
loss = 2.6364170480519533e-05
loss = 0.00036670322879217565
loss = 0.00016988598508760333
loss = 0.0003186354588251561
loss = 0.853135347366333
loss = 0.00025464320788159966
loss = 0.7942874431610107
loss = 0.0028612397145479918
loss = 5.008158041164279e-05
loss = 0.0017651939997449517
loss = 1.8198625184595585e-05
loss = 0.010189562104642391
loss = 0.002531670965254307
loss = 0.00034020302700810134
loss = 0.0017227899516001344
loss = 0.031220709905028343
loss = 0.011608488857746124
loss = 0.0007120745140127838
loss = 0.00218558800406754
loss = 0.004412917885929346
loss = 7.351090516749537e-06
loss = 0.1467079073190689
loss = 0.045458871871232986
loss = 6.03359512751922e-05
loss = 0.001995278988033533
loss = 0.000835711311083287
loss = 0.010836172848939896
loss = 3.973642748178463e-08
loss = 2.054317701549735e-05
loss = 0.00012242753291502595
loss = 0.0005958447000011802
loss = 7.051817374303937e-05
loss = 0.0017662850441411138
loss = 0.1336137354373932
loss = 0.002840188331902027
loss = 0.03480228781700134
loss = 6.755184926987567e-07
loss = 0.005649600178003311
loss = 0.020349815487861633
loss = 0.2558824121952057
loss = 0.10489466786384583
loss = 0.0008942783460952342
loss = 0.23729045689105988
loss = 4.239408372086473e-05
loss = 1.927213133967598e-06
loss = 1.2576289918797556e-05
loss = 0.000736534537281841
loss = 4.035045640193857e-05
loss = 0.00012522278120741248
loss = 0.0010887326207011938
loss = 0.00017925724387168884
loss = 0.0002847231808118522
loss = 1.6689272115399945e-06
loss = 2.5449569875490852e-05
loss = 5.5026084737619385e-05
loss = 0.0001412459387211129
loss = 0.4643245041370392
loss = 0.0005611909436993301
loss = 6.973614745220402e-06
loss = 0.003514151321724057
loss = 0.0023055870551615953
loss = 0.9849798083305359
loss = 2.860998392861802e-06
loss = 7.379407907137647e-05
loss = 0.10321275144815445
loss = 0.042997341603040695
loss = 0.05451123043894768
loss = 0.0001869718689704314
loss = 0.021826928481459618
loss = 0.13396267592906952
loss = 1.1960449228354264e-05
loss = 0.0005250743706710637
loss = 0.01142673660069704
loss = 0.25824835896492004
loss = 0.0008170935907401145
loss = 0.005464625079184771
loss = 0.00014213619579095393
loss = 0.0035211152862757444
loss = 5.086209512228379e-06
loss = 7.748585630906746e-07
loss = 0.000291389471385628
loss = 0.00011404594260966405
loss = 1.2441301345825195
loss = 2.441630385874305e-05
loss = 0.7978622913360596
loss = 0.0001581120304763317
loss = 0.3944404125213623
loss = 0.0407106839120388
loss = 0.000720302399713546
loss = 1.6966929251793772e-05
loss = 0.0071889800019562244
loss = 0.06141558289527893
loss = 0.005306243430823088
loss = 0.00029267233912833035
loss = 0.0016778492135927081
loss = 0.019128844141960144
loss = 1.2099409104848746e-05
loss = 0.009229423478245735
loss = 5.861033059773035e-06
loss = 6.159138479233661e-07
loss = 0.00012075958511559293
loss = 0.04896758869290352
loss = 0.004299555905163288
loss = 4.768365045038081e-07
loss = 0.2543694078922272
loss = 0.010811430402100086
loss = 0.0017558865947648883
loss = 0.008015835657715797
loss = 5.8882837038254365e-05
loss = 0.0024600569158792496
loss = 0.7266897559165955
loss = 0.11246353387832642
loss = 0.1511320322751999
loss = 0.00016732689982745796
loss = 0.006589385215193033
loss = 0.7746222615242004
loss = 0.012724027037620544
loss = 0.0013104681856930256
loss = 0.02192092128098011
loss = 0.3040107786655426
loss = 0.0013300616992637515
loss = 9.934105804632054e-08
loss = 0.043500866740942
loss = 1.6093184740384459e-06
loss = 2.5489067411399446e-05
loss = 0.006935186684131622
loss = 0.03803098574280739
loss = 0.27835187315940857
loss = 1.5228095054626465
loss = 0.0012867901241406798
loss = 0.34497880935668945
loss = 0.10022199898958206
loss = 0.0007775889826007187
loss = 1.9152372260577977e-05
loss = 0.003984746988862753
loss = 0.003023372031748295
loss = 0.053447697311639786
loss = 6.0468843003036454e-05
loss = 1.615716576576233
loss = 1.670252799987793
loss = 0.002541339723393321
loss = 3.248209031880833e-05
loss = 5.036299626226537e-05
loss = 6.245406257221475e-05
loss = 0.05973643437027931
loss = 0.00011171831283718348
loss = 4.870974225923419e-05
loss = 0.0006369815673679113
loss = 0.5421457290649414
loss = 0.00937948003411293
loss = 0.04399624466896057
loss = 0.018012793734669685
loss = 0.6511229872703552
loss = 0.05079354718327522
loss = 0.0005619658040814102
loss = 0.4284057319164276
loss = 0.015374410897493362
loss = 0.09843022376298904
loss = 0.0006899488507770002
loss = 0.28900280594825745
loss = 0.00011871914466610178
loss = 0.015376213937997818
loss = 3.317978553241119e-06
loss = 0.0005525459419004619
loss = 0.010112210176885128
loss = 0.5192679762840271
loss = 0.05490485206246376
loss = 0.014551791362464428
loss = 0.028811456635594368
loss = 0.0007133269100449979
loss = 0.11986741423606873
loss = 0.00023746788792777807
loss = 2.3841778329369845e-06
loss = 0.030365334823727608
loss = 7.70871338318102e-06
loss = 0.0012847880134359002
loss = 0.004414736293256283
loss = 0.004309301730245352
loss = 0.07210248708724976
loss = 1.1920696124434471e-05
loss = 0.009226005524396896
loss = 0.0009143519564531744
loss = 8.621102460892871e-05
loss = 0.00326627679169178
loss = 0.00022063804499339312
loss = 0.002598040970042348
loss = 0.013852551579475403
loss = 1.1483635717013385e-05
loss = 0.003775176592171192
loss = 0.011135016568005085
loss = 0.02580677717924118
loss = 0.000232942053116858
loss = 1.841180443763733
loss = 0.0004512715386226773
loss = 0.2414536476135254
loss = 0.0017996537499129772
loss = 8.206151687772945e-05
loss = 0.00026371271815150976
loss = 0.011210302822291851
loss = 1.0115472078323364
loss = 4.64058066427242e-05
loss = 2.360228063480463e-05
loss = 0.0013161542592570186
loss = 0.001308714970946312
loss = 0.0023800155613571405
loss = 0.017608964815735817
loss = 0.0020734930876642466
loss = 0.003146119648590684
loss = 0.0671987384557724
loss = 0.17141187191009521
loss = 0.7182514667510986
loss = 0.00445730471983552
loss = 0.02116345800459385
loss = 0.007149626966565847
loss = 0.00040686005377210677
loss = 0.34775689244270325
loss = 0.012127925641834736
loss = 0.009845093823969364
loss = 0.001654740422964096
loss = 0.14245785772800446
loss = 0.05765879154205322
loss = 0.0014548796461895108
loss = 0.002485560951754451
loss = 0.0005996168474666774
loss = 0.01812032051384449
loss = 2.4059254428721033e-05
loss = 0.00011108737089671195
loss = 0.5697094798088074
loss = 0.02314411662518978
loss = 0.004551562946289778
loss = 0.5215948820114136
loss = 0.001162496511824429
loss = 0.1965048462152481
loss = 0.00012746546417474747
loss = 0.0014895001659169793
loss = 0.32429155707359314
loss = 0.017167439684271812
loss = 0.0012204902013763785
loss = 0.00020112314086873084
loss = 0.0015745055861771107
loss = 0.0063496665097773075
loss = 0.00036380559322424233
loss = 0.0011808391427621245
loss = 0.09538456052541733
loss = 0.0004225012380629778
loss = 0.0001555614435346797
loss = 0.0007043415680527687
loss = 5.70213160244748e-06
loss = 0.005140070337802172
loss = 0.3053242862224579
loss = 0.0008398065692745149
loss = 0.002140964148566127
loss = 8.355791942449287e-05
loss = 1.1662262295430992e-05
loss = 0.016192855313420296
loss = 3.7550628348981263e-06
loss = 0.01915319822728634
loss = 0.03138735517859459
loss = 0.11912614107131958
loss = 0.0005284409853629768
loss = 0.0011022709077224135
loss = 0.05472392961382866
loss = 0.0020607339683920145
loss = 0.0016160322120413184
loss = 0.009925169870257378
loss = 0.00011517968960106373
loss = 0.2491394281387329
loss = 0.8953497409820557
loss = 0.10915559530258179
loss = 0.019588826224207878
loss = 0.03516216203570366
loss = 0.01492281537503004
loss = 0.0002933962387032807
loss = 0.18002577126026154
loss = 2.603097915649414
loss = 0.00013529040734283626
loss = 0.13795442879199982
loss = 0.000396644783904776
loss = 0.00012784429418388754
loss = 2.67809573415434e-05
loss = 0.017372071743011475
loss = 0.09654910117387772
loss = 0.004707640968263149
loss = 0.36741903424263
loss = 0.002239649649709463
loss = 0.029424354434013367
loss = 1.764227454259526e-05
loss = 0.0022050978150218725
loss = 0.2172120064496994
loss = 0.010771808214485645
loss = 0.012966829352080822
loss = 0.6016013622283936
loss = 0.5916733145713806
loss = 3.667523924377747e-05
loss = 7.620076212333515e-05
loss = 0.01845698617398739
loss = 0.021465713158249855
loss = 0.00038119839155115187
loss = 6.132773705758154e-05
loss = 0.21926987171173096
loss = 0.0026509033050388098
loss = 0.000762941490393132
loss = 0.0015451634535565972
loss = 7.390886821667664e-06
loss = 0.024919921532273293
loss = 1.7701871911413036e-05
loss = 0.002847082680091262
loss = 0.0006127253291197121
loss = 0.016141336411237717
loss = 0.286347359418869
loss = 0.019986599683761597
loss = 0.003550659865140915
loss = 0.0021648253314197063
loss = 0.0005438012885861099
loss = 0.0018481750739738345
loss = 6.077284342609346e-05
loss = 6.516722351079807e-06
loss = 0.0004017527389805764
loss = 2.1840903759002686
loss = 1.734448414936196e-05
loss = 7.58428213885054e-05
loss = 3.198754029654083e-06
loss = 0.010206147097051144
loss = 0.013076542876660824
loss = 0.00934797152876854
loss = 0.00014807381376158446
loss = 0.00591042498126626
loss = 3.576052040443756e-05
loss = 9.226248221239075e-05
loss = 0.010036655701696873
loss = 0.0063572232611477375
loss = 0.027102865278720856
loss = 0.00023404217790812254
loss = 4.847776835958939e-06
loss = 0.00018414384976495057
loss = 0.019391359761357307
loss = 0.002853671321645379
loss = 6.159136205496907e-07
loss = 0.0005253494600765407
loss = 0.0636233314871788
loss = 0.9944271445274353
loss = 0.0005263545317575336
loss = 0.4658268690109253
loss = 0.0010067322291433811
loss = 1.0331456223866553e-06
loss = 0.6146332621574402
loss = 0.00024376559304073453
loss = 0.0010349018266424537
loss = 0.0916135385632515
loss = 0.010485564358532429
loss = 1.1940603144466877e-05
loss = 5.08582015754655e-05
loss = 0.00011252024705754593
loss = 9.496854545432143e-06
loss = 1.6291888869091053e-06
loss = 0.00025152022135443985
loss = 0.0015363926067948341
loss = 0.0006729413289576769
loss = 7.530349830631167e-05
loss = 1.1608940362930298
loss = 0.08459759503602982
loss = 0.0005730338743887842
loss = 7.964141695993021e-05
loss = 0.015277508646249771
loss = 0.00040786832687444985
loss = 0.0002125203172909096
loss = 0.33035025000572205
loss = 0.0015384863363578916
loss = 4.8594301915727556e-05
loss = 0.4651411473751068
loss = 0.04203527048230171
loss = 0.012321331538259983
loss = 0.020686261355876923
loss = 0.005169963929802179
loss = 0.002193932421505451
loss = 0.0005699227913282812
loss = 6.553789717145264e-05
loss = 7.922479562694207e-05
loss = 2.3841773781896336e-06
loss = 0.19172395765781403
loss = 2.7078824132331647e-05
loss = 0.052656739950180054
loss = 0.8567518591880798
loss = 0.002815114101395011
loss = 7.94728478581419e-08
loss = 0.0008817285415716469
loss = 0.07786307483911514
loss = 0.002818600507453084
loss = 0.010525659658014774
loss = 3.5362874768907204e-05
loss = 0.004607330542057753
loss = 0.022732539102435112
loss = 0.000535976083483547
loss = 0.05216745659708977
loss = 1.2516928791228565e-06
loss = 0.02590055763721466
loss = 0.004717640113085508
loss = 7.372286199824885e-05
loss = 0.008333791978657246
loss = 0.004937899298965931
loss = 0.01204658579081297
loss = 0.0015813065692782402
loss = 0.0005308552645146847
loss = 0.0045421007089316845
loss = 0.0022803095635026693
loss = 0.000591166433878243
loss = 0.001617151778191328
loss = 0.04773247241973877
loss = 0.00035534289781935513
loss = 0.00016919506015256047
loss = 0.00011707923840731382
loss = 0.00016760510334279388
loss = 0.1310209482908249
loss = 0.529452919960022
loss = 1.7165739336633123e-05
loss = 0.18783821165561676
loss = 0.000384684739401564
loss = 0.01928596757352352
loss = 0.0013557838974520564
loss = 7.749861833872274e-05
loss = 0.01361134648323059
loss = 1.7881383485018887e-07
loss = 0.08806810528039932
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.968" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31250&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="221.78380072785657" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1262a0610&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9717833333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="1007.0871659185926" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
loss = 9.917518036672845e-05
loss = 0.6239315271377563
loss = 0.07572390139102936
loss = 6.755181516382436e-07
loss = 0.005210259463638067
loss = 3.3378330499544973e-06
loss = 0.01102446112781763
loss = 3.971334808738902e-05
loss = 0.007307321298867464
loss = 0.002431459492072463
loss = 0.0005667156656272709
loss = 0.02337346039712429
loss = 0.00025016613653860986
loss = 6.45427280687727e-05
loss = 0.040570586919784546
loss = 2.4239109279733384e-06
loss = 0.00030098482966423035
loss = 0.007417924702167511
loss = 0.006961234379559755
loss = 0.0008855937630869448
loss = 5.28631207998842e-05
loss = 0.0038567781448364258
loss = 3.564125290722586e-05
loss = 0.02875089831650257
loss = 8.046517905313522e-06
loss = 0.00027558571309782565
loss = 0.002213618950918317
loss = 0.0005909012979827821
loss = 3.6337074561743066e-05
loss = 2.5828485377132893e-06
loss = 0.0005873428308404982
loss = 0.0030366608407348394
loss = 0.002622091444209218
loss = 0.016697468236088753
loss = 0.00732301315292716
loss = 0.0008778904448263347
loss = 0.008058714680373669
loss = 0.0019808115903288126
loss = 1.780097773007583e-05
loss = 0.7066506743431091
loss = 0.000274425110546872
loss = 0.014977681450545788
loss = 2.3841728307161247e-06
loss = 1.212729573249817
loss = 9.178878826787695e-06
loss = 5.463670731842285e-06
loss = 0.0019072034629061818
loss = 1.6689251651769155e-06
loss = 0.00036982668098062277
loss = 0.012663361616432667
loss = 0.0001332256942987442
loss = 0.007989667356014252
loss = 1.4901100939823664e-06
loss = 0.0003679530927911401
loss = 0.06474828720092773
loss = 0.00010858711175387725
loss = 0.00010260340059176087
loss = 0.00010153129551326856
loss = 0.010121633298695087
loss = 3.917551293852739e-05
loss = 0.0007023999933153391
loss = 0.004686201456934214
loss = 0.0023911346215754747
loss = 9.885503095574677e-05
loss = 0.012869749218225479
loss = 0.015010454691946507
loss = 0.11655526608228683
loss = 0.053089216351509094
loss = 0.00034729731851257384
loss = 1.452306332794251e-05
loss = 0.26744726300239563
loss = 0.002081178827211261
loss = 9.417274668521713e-06
loss = 0.0025174093898385763
loss = 0.0021936336997896433
loss = 0.00026102340780198574
loss = 0.0016316667897626758
loss = 0.0031369132921099663
loss = 0.3045211732387543
loss = 0.006747283507138491
loss = 0.0015457519330084324
loss = 0.0004689217603299767
loss = 0.00917933788150549
loss = 0.011244668625295162
loss = 0.0006475482368841767
loss = 1.8973512851516716e-05
loss = 0.002534086350351572
loss = 0.06735114008188248
loss = 0.08111254125833511
loss = 0.0009405646123923361
loss = 0.009290304034948349
loss = 9.671101724961773e-05
loss = 6.139227934909286e-06
loss = 0.00021324398403521627
loss = 0.0008481686818413436
loss = 0.04003323242068291
loss = 0.09164270013570786
loss = 0.0001930548605741933
loss = 0.008943511173129082
loss = 0.1277778446674347
loss = 0.0008492631022818387
loss = 3.7050140235805884e-05
loss = 8.940676821111992e-07
loss = 1.4781329809920862e-05
loss = 1.7170023918151855
loss = 0.00630450202152133
loss = 0.06737105548381805
loss = 9.794817742658779e-06
loss = 2.0463250621105544e-05
loss = 0.4709274470806122
loss = 0.004539626184850931
loss = 0.018643701449036598
loss = 6.0795628087362275e-06
loss = 0.00046545927762053907
loss = 0.034825678914785385
loss = 1.6350973965018056e-05
loss = 0.005959686357527971
loss = 0.0004021576896775514
loss = 1.2556261026475113e-05
loss = 0.028388654813170433
loss = 0.03882865235209465
loss = 0.001150930067524314
loss = 0.0004687334585469216
loss = 0.00016109201533254236
loss = 1.706630428088829e-05
loss = 0.03559939190745354
loss = 5.702082944480935e-06
loss = 0.3480552136898041
loss = 0.0011447611032053828
loss = 6.484206824097782e-05
loss = 0.019519632682204247
loss = 0.09621653705835342
loss = 0.007743673399090767
loss = 0.0019253873033449054
loss = 0.012359414249658585
loss = 0.019671473652124405
loss = 0.0005438682273961604
loss = 0.006646317895501852
loss = 0.02027992345392704
loss = 0.009102173149585724
loss = 0.002300003543496132
loss = 0.023888448253273964
loss = 6.059710813133279e-06
loss = 0.06495945900678635
loss = 6.12424555583857e-05
loss = 3.436848783167079e-05
loss = 0.028060728684067726
loss = 5.980293281027116e-06
loss = 0.06648152321577072
loss = 0.0007891645655035973
loss = 2.155600486730691e-05
loss = 0.00031289542675949633
loss = 8.661447645863518e-05
loss = 0.0002673043345566839
loss = 0.00046624429523944855
loss = 0.00011932567576877773
loss = 3.496608405839652e-05
loss = 9.429195779375732e-05
loss = 2.5033803012775024e-06
loss = 0.0022107490804046392
loss = 0.0006198246846906841
loss = 0.018896380439400673
loss = 0.009733336046338081
loss = 0.0004215276276227087
loss = 0.0036607563961297274
loss = 0.0072207520715892315
loss = 1.589456815054291e-07
loss = 0.35967329144477844
loss = 5.3047738219902385e-06
loss = 0.013541926629841328
loss = 2.384168965363642e-06
loss = 0.00013708187907468528
loss = 0.7859814763069153
loss = 0.6196038126945496
loss = 4.8353442252846435e-05
loss = 0.0002851389581337571
loss = 0.8398497700691223
loss = 7.914535672171041e-05
loss = 0.0040829116478562355
loss = 0.20788104832172394
loss = 0.00627482682466507
loss = 0.00345443282276392
loss = 0.002895682118833065
loss = 5.4172243835637346e-05
loss = 0.05164690688252449
loss = 5.046488240623148e-06
loss = 0.004202419891953468
loss = 0.031802717596292496
loss = 1.2318272410993814e-06
loss = 8.336643077200279e-05
loss = 0.05424468219280243
loss = 0.0007356179994530976
loss = 0.511042594909668
loss = 0.01716594211757183
loss = 1.307297952735098e-05
loss = 0.18644194304943085
loss = 0.039317745715379715
loss = 2.5073009965126403e-05
loss = 5.284920916892588e-06
loss = 0.006833640392869711
loss = 8.602731213613879e-06
loss = 0.005746984388679266
loss = 0.02016586624085903
loss = 0.004457785282284021
loss = 0.09395157545804977
loss = 0.34356045722961426
loss = 0.21733327209949493
loss = 3.2880361686693504e-05
loss = 0.0003777857928071171
loss = 0.006393901538103819
loss = 0.00726848840713501
loss = 0.19759255647659302
loss = 4.080742291989736e-05
loss = 0.1129378154873848
loss = 0.003688620636239648
loss = 0.0008716208976693451
loss = 8.344632078660652e-07
loss = 1.0858427286148071
loss = 0.00022146930859889835
loss = 0.009145638905465603
loss = 0.7859652638435364
loss = 0.0002603705506771803
loss = 0.6859201788902283
loss = 4.867667939834064e-06
loss = 0.0040682717226445675
loss = 1.9073403336733463e-06
loss = 0.005810856819152832
loss = 1.1312226057052612
loss = 0.3156301975250244
loss = 0.00020044959092047065
loss = 0.2555699646472931
loss = 0.010417758487164974
loss = 0.0014350685523822904
loss = 0.0007264759042300284
loss = 5.556216274271719e-05
loss = 0.007133382838219404
loss = 0.0018542137695476413
loss = 0.34136128425598145
loss = 3.914014541805955e-06
loss = 0.005996271967887878
loss = 0.49575743079185486
loss = 0.0012439122656360269
loss = 7.887585525168106e-06
loss = 0.0003994096477981657
loss = 0.005446743220090866
loss = 0.0013335132971405983
loss = 0.00014477675722446293
loss = 0.11995496600866318
loss = 0.2820732891559601
loss = 0.0006155591108836234
loss = 1.8834316506399773e-05
loss = 0.22917215526103973
loss = 0.0024691629223525524
loss = 0.26856178045272827
loss = 0.0028244610875844955
loss = 0.0019136726623401046
loss = 0.0005466389120556414
loss = 0.1281018704175949
loss = 0.0001508984132669866
loss = 0.39539822936058044
loss = 1.3112779015500564e-05
loss = 0.12182263284921646
loss = 0.0
loss = 0.17126654088497162
loss = 0.011092998087406158
loss = 0.22511066496372223
loss = 0.036933060735464096
loss = 0.007662224117666483
loss = 0.008491466753184795
loss = 0.06216235086321831
loss = 0.21817009150981903
loss = 0.0001985910494113341
loss = 0.04035594314336777
loss = 0.00029872971936129034
loss = 0.3278641104698181
loss = 6.847298209322616e-05
loss = 0.0004393807612359524
loss = 0.001116396626457572
loss = 0.04806242510676384
loss = 0.0067823841236531734
loss = 0.4531525671482086
loss = 0.0011135757667943835
loss = 0.001479814643971622
loss = 2.203332223871257e-05
loss = 0.0010926065733656287
loss = 2.7616599709290313e-06
loss = 0.0021712693851441145
loss = 0.0001734251854941249
loss = 0.0011162194423377514
loss = 0.008467625826597214
loss = 3.186754838679917e-05
loss = 0.0004704988095909357
loss = 0.6056368947029114
loss = 3.2186301268666284e-06
loss = 2.2451010863733245e-06
loss = 0.003986461088061333
loss = 0.0014527039602398872
loss = 8.940683073888067e-07
loss = 0.9358651041984558
loss = 0.11578486114740372
loss = 0.00519822770729661
loss = 1.7660828828811646
loss = 0.00031166677945293486
loss = 0.1314663141965866
loss = 0.004732853267341852
loss = 0.0012250123545527458
loss = 0.00516870291903615
loss = 0.10100328922271729
loss = 0.03276059031486511
loss = 0.0006894203252159059
loss = 0.004291351418942213
loss = 0.0018609198741614819
loss = 0.0006387507892213762
loss = 2.5985704269260168e-05
loss = 0.031683892011642456
loss = 0.0005478154052980244
loss = 2.8013946575811133e-06
loss = 0.0644768700003624
loss = 1.0011546611785889
loss = 0.34618064761161804
loss = 0.31205329298973083
loss = 0.5308979749679565
loss = 0.8202946186065674
loss = 1.1082870960235596
loss = 0.028212079778313637
loss = 0.48918285965919495
loss = 0.00016014104767236859
loss = 0.728145182132721
loss = 0.0017090094042941928
loss = 0.23200954496860504
loss = 1.9868213740892315e-08
loss = 0.0005018083611503243
loss = 0.04004170373082161
loss = 0.00019564025569707155
loss = 4.406326843309216e-05
loss = 0.0002820293011609465
loss = 1.1802109479904175
loss = 0.01643637754023075
loss = 0.12041079998016357
loss = 0.020764710381627083
loss = 0.00304692261852324
loss = 0.008065557107329369
loss = 0.002242123009636998
loss = 0.00023815764870960265
loss = 0.00017209374345839024
loss = 0.000818818632978946
loss = 0.015292574651539326
loss = 0.0006889000651426613
loss = 0.0009484973852522671
loss = 0.0002811875019688159
loss = 0.37922072410583496
loss = 0.004657947923988104
loss = 0.597292423248291
loss = 0.00011172595986863598
loss = 0.001639644498936832
loss = 0.000218120330828242
loss = 1.64703178597847e-05
loss = 0.0001705479371594265
loss = 3.397444515940151e-06
loss = 0.0004194871580693871
loss = 1.1891076564788818
loss = 0.4027233123779297
loss = 0.0042207371443510056
loss = 0.00013079239579383284
loss = 5.3461233619600534e-05
loss = 0.0002921878476627171
loss = 0.03445648401975632
loss = 0.00018650344281923026
loss = 1.9509539924911223e-05
loss = 0.02342231571674347
loss = 0.25757214426994324
loss = 0.000253828038694337
loss = 0.000869160343427211
loss = 0.12810145318508148
loss = 0.06749323755502701
loss = 0.0010405213106423616
loss = 0.0017574356170371175
loss = 0.00483799958601594
loss = 0.04966669902205467
loss = 0.890376091003418
loss = 0.18658322095870972
loss = 0.009906587190926075
loss = 0.0028087105602025986
loss = 6.728951120749116e-05
loss = 0.03183887153863907
loss = 0.002671034773811698
loss = 0.02897047810256481
loss = 0.00030304075335152447
loss = 0.0007031448767520487
loss = 3.437184204813093e-06
loss = 0.0004173996567260474
loss = 0.024167926982045174
loss = 0.005781327839940786
loss = 4.799874659511261e-05
loss = 0.0009001880534924567
loss = 0.004501281771808863
loss = 0.00012647775292862207
loss = 0.00031648389995098114
loss = 0.052144601941108704
loss = 0.0029122671112418175
loss = 0.0009553415584377944
loss = 0.6341137290000916
loss = 0.20705564320087433
loss = 0.0034129787236452103
loss = 0.018936753273010254
loss = 0.0010838323505595326
loss = 0.0016712850192561746
loss = 0.016647983342409134
loss = 1.7565969228744507
loss = 0.0001338040892733261
loss = 3.359390757395886e-05
loss = 0.10413780808448792
loss = 0.010206013917922974
loss = 1.9549850549083203e-05
loss = 0.0762486681342125
loss = 1.815896939660888e-05
loss = 0.0
loss = 3.1789113563718274e-07
loss = 2.8549402486532927e-05
loss = 6.556401785928756e-06
loss = 0.006396863143891096
loss = 0.0030976899433881044
loss = 2.268826028739568e-05
loss = 0.03763807192444801
loss = 0.0297174621373415
loss = 2.1118650693097152e-05
loss = 0.0003436761617194861
loss = 0.0005429720622487366
loss = 0.0005015383358113468
loss = 0.43926307559013367
loss = 0.00017334026051685214
loss = 0.0032754475250840187
loss = 0.11993146687746048
loss = 0.0012573606800287962
loss = 0.0008509462350048125
loss = 0.00619010254740715
loss = 0.009698063135147095
loss = 0.00017114041838794947
loss = 0.01952114887535572
loss = 6.079628747102106e-06
loss = 0.0001711643853923306
loss = 0.10058721154928207
loss = 3.0199471439118497e-06
loss = 4.569630164041882e-06
loss = 2.781549142127915e-07
loss = 0.003268845146521926
loss = 0.00043978754547424614
loss = 0.0016347711207345128
loss = 0.04445507749915123
loss = 0.34664249420166016
loss = 0.18496985733509064
loss = 0.013790464960038662
loss = 0.11745178699493408
loss = 7.560931408079341e-05
loss = 0.0006802466814406216
loss = 0.0018188097747042775
loss = 0.5719001293182373
loss = 0.004522805102169514
loss = 4.301141234464012e-05
loss = 1.0172342626901809e-05
loss = 0.001576308161020279
loss = 0.006707068998366594
loss = 0.03310886025428772
loss = 0.010447618551552296
loss = 0.0024753606412559748
loss = 0.006652366369962692
loss = 0.009142243303358555
loss = 0.9446126818656921
loss = 3.4448628866812214e-05
loss = 0.0068924822844564915
loss = 8.364369023183826e-06
loss = 0.006587345618754625
loss = 0.004999952390789986
loss = 0.05974653735756874
loss = 0.08788332343101501
loss = 2.3760927433613688e-05
loss = 8.404043910559267e-06
loss = 0.07054077833890915
loss = 0.05864885076880455
loss = 1.2715635193671915e-06
loss = 0.004214353393763304
loss = 0.003120436565950513
loss = 0.0038970739115029573
loss = 0.4999774992465973
loss = 0.003000750904902816
loss = 0.3662277162075043
loss = 1.752323987602722e-05
loss = 5.8303008700022474e-05
loss = 0.00042716553434729576
loss = 4.7683647608209867e-07
loss = 0.01785341463983059
loss = 0.00015623790386598557
loss = 0.004332253243774176
loss = 0.27423131465911865
loss = 0.31434205174446106
loss = 0.00041950648301281035
loss = 4.223723954055458e-05
loss = 0.9919541478157043
loss = 0.0033409874886274338
loss = 0.11903920769691467
loss = 0.0005742930807173252
loss = 0.0032972435001283884
loss = 0.38644465804100037
loss = 0.0008387265261262655
loss = 0.02861226536333561
loss = 0.0042823669500648975
loss = 0.0009511595126241446
loss = 0.10797134041786194
loss = 7.544749678345397e-05
loss = 0.007287866901606321
loss = 0.0008133046212606132
loss = 0.008411460556089878
loss = 0.02335973083972931
loss = 0.18340973556041718
loss = 0.004670186899602413
loss = 0.00011310968693578616
loss = 0.001657277811318636
loss = 0.1285361796617508
loss = 0.0023039057850837708
loss = 8.619032450951636e-05
loss = 0.003138020634651184
loss = 0.46366798877716064
loss = 0.00643056258559227
loss = 0.02788952738046646
loss = 0.05086870864033699
loss = 2.7815378871309804e-06
loss = 0.0023645542096346617
loss = 0.0004261386638972908
loss = 0.09245026111602783
loss = 0.06446842104196548
loss = 0.0005841833190061152
loss = 3.894132078130497e-06
loss = 0.0023564230650663376
loss = 0.00018604955403134227
loss = 6.374501390382648e-05
loss = 0.09143947809934616
loss = 0.048193275928497314
loss = 0.003894777037203312
loss = 1.9073410157943727e-06
loss = 0.0005051125190220773
loss = 0.3560924530029297
loss = 1.3668800420418847e-05
loss = 1.1920926823449918e-07
loss = 0.0005521654966287315
loss = 1.9470737697702134e-06
loss = 4.626679219654761e-05
loss = 0.0002767685509752482
loss = 0.6335708498954773
loss = 0.015546773560345173
loss = 0.001060917042195797
loss = 0.03098880685865879
loss = 2.1893340090173297e-05
loss = 5.602792953141034e-06
loss = 0.0012559803435578942
loss = 0.0883488729596138
loss = 9.536724974168465e-07
loss = 0.05710792914032936
loss = 0.0046616848558187485
loss = 0.010941055603325367
loss = 0.02966839261353016
loss = 5.6027893151622266e-06
loss = 0.22911904752254486
loss = 0.002022781176492572
loss = 1.0033200851466972e-05
loss = 7.409779209410772e-05
loss = 8.036952203838155e-05
loss = 0.13257135450839996
loss = 0.0005206709611229599
loss = 0.1994829624891281
loss = 3.1192905680654803e-06
loss = 0.35351788997650146
loss = 0.0035225122701376677
loss = 8.241389878094196e-05
loss = 0.0010233336361125112
loss = 0.03394844010472298
loss = 0.001436054240912199
loss = 0.01680184155702591
loss = 0.00016445282381027937
loss = 0.000841544650029391
loss = 0.00020778505131602287
loss = 0.005870623979717493
loss = 0.0011235898127779365
loss = 0.000503971241414547
loss = 0.2492830902338028
loss = 1.4953492879867554
loss = 5.989263081573881e-05
loss = 0.00010614477650960907
loss = 4.8670019168639556e-05
loss = 0.054007451981306076
loss = 0.0004309315700083971
loss = 0.00016001908807083964
loss = 0.020352158695459366
loss = 0.2345859259366989
loss = 0.5315161347389221
loss = 3.993489372078329e-06
loss = 0.017077220603823662
loss = 0.003964305389672518
loss = 2.7680165767669678
loss = 0.008578852750360966
loss = 8.639917359687388e-05
loss = 0.5573539137840271
loss = 0.048440199345350266
loss = 0.10773850232362747
loss = 0.0011003665858879685
loss = 0.24754858016967773
loss = 0.4334445297718048
loss = 0.002989670494571328
loss = 0.009823901578783989
loss = 0.03347845375537872
loss = 0.0019011832773685455
loss = 1.1523546845637611e-06
loss = 0.00035610151826404035
loss = 0.7739974856376648
loss = 0.0017070529283955693
loss = 0.00023021327797323465
loss = 0.03411957621574402
loss = 0.00028462952468544245
loss = 0.08263032138347626
loss = 0.0002867598959710449
loss = 2.382102502451744e-05
loss = 0.002492239698767662
loss = 0.03293139487504959
loss = 0.00026433906168676913
loss = 0.25075504183769226
loss = 0.001871957560069859
loss = 0.000746975711081177
loss = 0.015553758479654789
loss = 0.0005559978890232742
loss = 0.0011987453326582909
loss = 0.0016142387175932527
loss = 0.002423578640446067
loss = 0.02596917189657688
loss = 2.797323941194918e-05
loss = 0.004113273695111275
loss = 0.0012476644478738308
loss = 0.0017742915078997612
loss = 9.684928954811767e-05
loss = 0.0769905298948288
loss = 0.0006135530420579016
loss = 0.0003401592548470944
loss = 3.9731694414513186e-05
loss = 0.015222067944705486
loss = 0.011232427321374416
loss = 0.003582632401958108
loss = 0.07348589599132538
loss = 0.0008873192709870636
loss = 0.4298950135707855
loss = 0.00033847984741441905
loss = 0.008741981349885464
loss = 0.002312441822141409
loss = 0.3528832495212555
loss = 0.06513873487710953
loss = 0.0006826312164776027
loss = 1.133371114730835
loss = 0.013894530944526196
loss = 0.13967128098011017
loss = 1.553658330522012e-05
loss = 0.016956279054284096
loss = 0.0009980456670746207
loss = 0.04581913724541664
loss = 0.013242143206298351
loss = 6.772323831683025e-05
loss = 0.0019171861931681633
loss = 0.3133337199687958
loss = 0.027989095076918602
loss = 0.0049132732674479485
loss = 0.39411661028862
loss = 0.0006158036994747818
loss = 0.0010539648355916142
loss = 0.17469914257526398
loss = 1.631106533750426e-05
loss = 2.198336362838745
loss = 0.0009557417943142354
loss = 0.02541661448776722
loss = 1.1553016901016235
loss = 0.004863046109676361
loss = 0.009240813553333282
loss = 0.006309479009360075
loss = 0.3209112584590912
loss = 0.002491287887096405
loss = 0.00032863931846804917
loss = 0.02228175289928913
loss = 0.00013892694551032037
loss = 0.3527563810348511
loss = 0.023595556616783142
loss = 0.005394601728767157
loss = 0.04538872465491295
loss = 0.020692644640803337
loss = 0.0012390491319820285
loss = 0.0007681432762183249
loss = 0.6578769683837891
loss = 0.004837398882955313
loss = 1.8079606888932176e-05
loss = 0.08441823720932007
loss = 0.0011922188568860292
loss = 0.007979181595146656
loss = 0.1519145369529724
loss = 0.13482248783111572
loss = 4.5096432586433366e-05
loss = 0.0001297012931900099
loss = 0.31979864835739136
loss = 0.0027143738698214293
loss = 0.0008373327436856925
loss = 0.009567084722220898
loss = 0.013968168757855892
loss = 0.0024967414792627096
loss = 0.0022370005026459694
loss = 3.3482325077056885
loss = 0.02117239683866501
loss = 0.0018001403659582138
loss = 0.0283083226531744
loss = 0.0006751270848326385
loss = 7.6293003985483665e-06
loss = 0.0014202361926436424
loss = 0.11628102511167526
loss = 0.8719813227653503
loss = 0.000212064987863414
loss = 3.6756080135091906e-06
loss = 0.0010040758643299341
loss = 0.0012656745966523886
loss = 0.49065402150154114
loss = 0.11005663126707077
loss = 0.0002676034637261182
loss = 0.5515986084938049
loss = 0.01697295717895031
loss = 8.611357043264434e-05
loss = 0.0011223949259147048
loss = 0.014117936603724957
loss = 0.6778626441955566
loss = 0.10958439856767654
loss = 0.02488933503627777
loss = 0.11150903254747391
loss = 0.00010899472545133904
loss = 0.24849961698055267
loss = 0.0014355579623952508
loss = 0.03519679978489876
loss = 9.277968638343737e-05
loss = 0.0002922702406067401
loss = 0.38888445496559143
loss = 0.002307708142325282
loss = 0.014879683963954449
loss = 0.0013187503209337592
loss = 3.933861989935394e-06
loss = 2.6821944629773498e-06
loss = 0.01909918338060379
loss = 0.01977643184363842
loss = 0.03232267498970032
loss = 0.0002209837402915582
loss = 0.10124670714139938
loss = 5.761772285950428e-07
loss = 0.007055794354528189
loss = 2.463652663209359e-06
loss = 0.011501667089760303
loss = 0.000244616880081594
loss = 0.012673561461269855
loss = 0.0003676429914776236
loss = 0.0039237854070961475
loss = 0.01461839023977518
loss = 0.5599203705787659
loss = 0.010695413686335087
loss = 0.0021192682906985283
loss = 0.8966488242149353
loss = 0.003301474265754223
loss = 0.00032673272653482854
loss = 0.0865621268749237
loss = 0.01117520872503519
loss = 0.007257872726768255
loss = 0.00026714394334703684
loss = 0.2581387460231781
loss = 0.7173421382904053
loss = 0.003612977685406804
loss = 0.0001539376244181767
loss = 0.025463275611400604
loss = 0.08757761120796204
loss = 0.14329807460308075
loss = 0.002458303002640605
loss = 0.3517741858959198
loss = 0.00011704338248819113
loss = 0.00035451355506666005
loss = 4.4776952563552186e-05
loss = 0.0032049904111772776
loss = 8.679000893607736e-05
loss = 0.650916576385498
loss = 0.980681836605072
loss = 0.7376232743263245
loss = 0.05149799957871437
loss = 0.0010089775314554572
loss = 0.00046262945397756994
loss = 0.001118961488828063
loss = 0.004629969596862793
loss = 0.003126290626823902
loss = 0.04839708283543587
loss = 0.0021329212468117476
loss = 0.3775651752948761
loss = 0.03652532026171684
loss = 0.021770479157567024
loss = 0.007394436746835709
loss = 0.062414947897195816
loss = 0.19193701446056366
loss = 3.973642748178463e-08
loss = 0.000574698147829622
loss = 0.799953043460846
loss = 0.1143159344792366
loss = 0.007648548111319542
loss = 5.028325904277153e-05
loss = 0.001515517826192081
loss = 0.0001425736118108034
loss = 0.0756383165717125
loss = 5.137520201969892e-05
loss = 0.003098019165918231
loss = 0.012895234860479832
loss = 0.008711325004696846
loss = 0.0013555986806750298
loss = 6.813855725340545e-05
loss = 0.07634605467319489
loss = 0.0003826601605396718
loss = 0.34666144847869873
loss = 0.0180706474930048
loss = 0.0004379246092867106
loss = 0.18149864673614502
loss = 0.4487917125225067
loss = 0.0032000094652175903
loss = 0.00025058211758732796
loss = 0.36916592717170715
loss = 0.5544267296791077
loss = 0.0900345966219902
loss = 0.004914583172649145
loss = 3.5203145671403036e-05
loss = 0.0009727755677886307
loss = 0.42208048701286316
loss = 0.00193740741815418
loss = 0.003839700249955058
loss = 0.00014986445603426546
loss = 9.61597925197566e-06
loss = 0.9237049221992493
loss = 8.999920828500763e-05
loss = 2.781548289476632e-07
loss = 0.00039171657408587635
loss = 0.007519850041717291
loss = 0.00667359447106719
loss = 0.2714058458805084
loss = 0.001122006680816412
loss = 0.6663943529129028
loss = 0.0001638024696148932
loss = 0.010718380101025105
loss = 1.9868213740892315e-08
loss = 4.6846933400956914e-05
loss = 0.06568741053342819
loss = 0.0004122213285882026
loss = 0.0006978907040320337
loss = 0.0013426374644041061
loss = 0.003456261707469821
loss = 0.00011634796828730032
loss = 4.0928166527010035e-06
loss = 0.5806604027748108
loss = 0.021130487322807312
loss = 0.09861928969621658
loss = 0.017793606966733932
loss = 0.0009840434649959207
loss = 0.16822367906570435
loss = 2.761519863270223e-05
loss = 2.940349622804206e-05
loss = 0.06147955358028412
loss = 0.0033762529492378235
loss = 0.0018751723691821098
loss = 0.44964393973350525
loss = 0.00027192255947738886
loss = 0.00032962579280138016
loss = 0.14071764051914215
loss = 8.496277587255463e-05
loss = 1.4503760894513107e-06
loss = 0.004146425984799862
loss = 0.0026688603684306145
loss = 7.512477895943448e-05
loss = 6.681089143967256e-05
loss = 0.0076653906144201756
loss = 0.0007092528976500034
loss = 0.43254998326301575
loss = 0.0013376543065533042
loss = 0.011559656821191311
loss = 2.781549142127915e-07
loss = 0.05345700681209564
loss = 8.43242451082915e-05
loss = 0.00021744209516327828
loss = 1.7682676798358443e-06
loss = 0.1695333570241928
loss = 0.015247204340994358
loss = 0.00039916925015859306
loss = 7.103363168425858e-05
loss = 1.7881387748275301e-07
loss = 0.0005806971457786858
loss = 0.00017770251724869013
loss = 1.65696837939322e-05
loss = 0.4557467997074127
loss = 0.019202908501029015
loss = 0.0020422886591404676
loss = 0.009105469100177288
loss = 0.1785563975572586
loss = 0.06961096078157425
loss = 0.004163344856351614
loss = 0.005663532763719559
loss = 0.018066255375742912
loss = 0.055545225739479065
loss = 0.013064466416835785
loss = 0.0023695724084973335
loss = 0.0001234388182638213
loss = 0.0
loss = 0.0001300700387218967
loss = 0.0001247257023351267
loss = 0.5446420311927795
loss = 0.0003114212886430323
loss = 0.0008231165702454746
loss = 7.271699359989725e-06
loss = 0.0011416053166612983
loss = 0.0772390365600586
loss = 0.0006990853580646217
loss = 0.0012521340977400541
loss = 0.466746062040329
loss = 0.002073190873488784
loss = 0.0006575887673534453
loss = 0.034232448786497116
loss = 0.002775360131636262
loss = 0.47044637799263
loss = 0.03497721999883652
loss = 0.011939466930925846
loss = 0.0008415078627876937
loss = 0.0016825584461912513
loss = 0.025595756247639656
loss = 0.024527376517653465
loss = 0.0014338186010718346
loss = 9.873564704321325e-05
loss = 0.0010757308918982744
loss = 0.007989308796823025
loss = 0.02229302190244198
loss = 0.0005506273009814322
loss = 6.1391656345222145e-06
loss = 0.033103134483098984
loss = 0.008429523557424545
loss = 0.0012795442016795278
loss = 0.6962547302246094
loss = 0.00761118670925498
loss = 0.0020674436818808317
loss = 8.052628982113674e-05
loss = 5.337760740076192e-05
loss = 0.0001557570358272642
loss = 0.012678868137300014
loss = 0.0038411652203649282
loss = 2.6680912924348377e-05
loss = 1.0526514053344727
loss = 8.134189556585625e-05
loss = 0.00115368387196213
loss = 0.0025257787201553583
loss = 1.6847896404215135e-05
loss = 0.2449006587266922
loss = 0.07481224089860916
loss = 0.0014051013858988881
loss = 7.975267362780869e-05
loss = 0.004939754027873278
loss = 1.0530120562179945e-06
loss = 0.00020994385704398155
loss = 0.11544518917798996
loss = 0.000220005982555449
loss = 0.00010017969907494262
loss = 0.00013379857409745455
loss = 0.00019189156591892242
loss = 0.20045019686222076
loss = 0.0655272975564003
loss = 0.27271410822868347
loss = 0.001234783441759646
loss = 0.28654447197914124
loss = 0.019526029005646706
loss = 0.0003532581031322479
loss = 0.27127501368522644
loss = 0.016313502565026283
loss = 0.0025639457162469625
loss = 0.0012891957303509116
loss = 2.411881177977193e-05
loss = 1.641069866309408e-05
loss = 0.15726065635681152
loss = 0.001289384556002915
loss = 0.001105945440940559
loss = 0.03925331309437752
loss = 0.00013597401266451925
loss = 0.2851264774799347
loss = 0.0017975723603740335
loss = 0.0014930242905393243
loss = 0.01906980574131012
loss = 0.002856932580471039
loss = 0.00024501350708305836
loss = 2.3539485931396484
loss = 0.003672488732263446
loss = 0.0015488596400246024
loss = 0.00177739595528692
loss = 0.03446931764483452
loss = 0.005035103764384985
loss = 0.015974348410964012
loss = 1.650988451729063e-05
loss = 1.4902416467666626
loss = 0.0506589449942112
loss = 0.00028613852919079363
loss = 0.0072821639478206635
loss = 1.6045705080032349
loss = 0.00010280200513079762
loss = 2.086150743707549e-06
loss = 0.0022056580055505037
loss = 0.0012866416946053505
loss = 0.00522352522239089
loss = 0.0020912690088152885
loss = 0.0
loss = 0.00015361297118943185
loss = 6.831282371422276e-05
loss = 0.003955076448619366
loss = 0.2151585817337036
loss = 0.004834616091102362
loss = 0.0006108402158133686
loss = 1.6093177919174195e-06
loss = 0.00040640728548169136
loss = 0.014761158265173435
loss = 0.005029598716646433
loss = 1.5000204257376026e-05
loss = 0.00013090013817418367
loss = 0.0039191534742712975
loss = 0.3003375232219696
loss = 6.936566933291033e-05
loss = 1.112616359932872e-06
loss = 1.0923820734024048
loss = 0.01168199721723795
loss = 0.014924143441021442
loss = 0.00011119263217551634
loss = 0.0028340236749500036
loss = 0.19232459366321564
loss = 0.00018258794443681836
loss = 0.0029502969700843096
loss = 0.05122116580605507
loss = 9.73514215729665e-06
loss = 0.03071853704750538
loss = 0.3939743936061859
loss = 0.0006241519586183131
loss = 0.0010086032561957836
loss = 1.9868213740892315e-08
loss = 0.0010665729641914368
loss = 0.03029109351336956
loss = 7.24834026186727e-05
loss = 0.35623589158058167
loss = 7.05172642483376e-05
loss = 0.0034399041905999184
loss = 0.014854658395051956
loss = 0.0001506452535977587
loss = 1.782083563739434e-05
loss = 9.573745774105191e-05
loss = 1.2516941296780715e-06
loss = 0.0
loss = 2.604617657198105e-05
loss = 1.3331042282516137e-05
loss = 4.0845010516932234e-05
loss = 0.0009269522852264345
loss = 0.0013051467249169946
loss = 0.011003888212144375
loss = 2.2131722289486788e-05
loss = 0.02581033669412136
loss = 0.00023728459200356156
loss = 0.005026175174862146
loss = 0.0002871016622520983
loss = 0.0003403267764952034
loss = 0.535079836845398
loss = 8.761746357777156e-06
loss = 0.7506147027015686
loss = 0.0010776439448818564
loss = 0.0007648635655641556
loss = 0.01954389177262783
loss = 0.2562415301799774
loss = 0.0020821739453822374
loss = 0.01567133143544197
loss = 0.0009444763418287039
loss = 0.0004368060326669365
loss = 0.009865340776741505
loss = 0.14565792679786682
loss = 0.00012488453648984432
loss = 8.812444866634905e-05
loss = 1.6609288650215603e-05
loss = 0.0014989833580330014
loss = 0.0024665941018611193
loss = 1.8128489255905151
loss = 0.00013423518976196647
loss = 0.010120323859155178
loss = 2.6424525003676536e-06
loss = 3.2085477869259194e-05
loss = 0.0002672159462235868
loss = 1.2099252939224243
loss = 0.5125833749771118
loss = 0.0006008130731061101
loss = 0.0005222185864113271
loss = 0.03175165876746178
loss = 0.004232064820826054
loss = 0.025688648223876953
loss = 4.811714461538941e-05
loss = 1.3768287317361683e-05
loss = 0.006137335207313299
loss = 0.00011174313840456307
loss = 0.018395522609353065
loss = 0.010065743699669838
loss = 0.015878548845648766
loss = 0.2674504816532135
loss = 0.35210978984832764
loss = 0.0038695570547133684
loss = 0.00016710827185306698
loss = 0.00035593131906352937
loss = 0.2508552074432373
loss = 6.253910396480933e-05
loss = 0.085262231528759
loss = 6.637305341428146e-05
loss = 0.003953947685658932
loss = 0.00044531471212394536
loss = 0.0005184926558285952
loss = 0.04357871040701866
loss = 0.3945208489894867
loss = 0.05199916660785675
loss = 0.7109667658805847
loss = 0.10373526811599731
loss = 0.34495866298675537
loss = 9.568135283188894e-05
loss = 0.00023424126266036183
loss = 2.77737472060835e-05
loss = 4.4183601858094335e-05
loss = 0.002349111484363675
loss = 0.023686179891228676
loss = 0.07494820654392242
loss = 0.058142129331827164
loss = 0.015269230119884014
loss = 0.0012172989081591368
loss = 3.1590361686539836e-06
loss = 0.20473350584506989
loss = 0.010102790780365467
loss = 0.07233687490224838
loss = 0.0019044690998271108
loss = 0.03445906937122345
loss = 0.16231390833854675
loss = 0.00011984322918578982
loss = 1.9669419089041185e-06
loss = 0.02834402024745941
loss = 0.00031356370891444385
loss = 0.006530719343572855
loss = 0.049582432955503464
loss = 0.12188076227903366
loss = 0.01678789034485817
loss = 0.3522960841655731
loss = 0.00023214130487758666
loss = 0.0006978787132538855
loss = 0.0014451040187850595
loss = 1.3132702406437602e-05
loss = 0.09138835221529007
loss = 0.051827091723680496
loss = 1.7086165826185606e-05
loss = 0.03337493911385536
loss = 3.73893890355248e-05
loss = 1.3907744289554103e-07
loss = 1.2138758897781372
loss = 4.674336014431901e-05
loss = 0.0004296719853300601
loss = 0.021817872300744057
loss = 0.041152097284793854
loss = 1.0393682718276978
loss = 0.00036451892810873687
loss = 0.0001315626286668703
loss = 0.13182781636714935
loss = 9.178940672427416e-06
loss = 0.00011684552737278864
loss = 0.0007145340205170214
loss = 0.002850006567314267
loss = 0.3642933666706085
loss = 3.93768677895423e-05
loss = 0.007329131010919809
loss = 0.0839790478348732
loss = 0.000706048624124378
loss = 0.00040922313928604126
loss = 0.2313830405473709
loss = 8.13285878393799e-05
loss = 0.0038453664164990187
loss = 0.02018553577363491
loss = 0.038707807660102844
loss = 7.36633810447529e-05
loss = 0.00032147965976037085
loss = 0.001467988477088511
loss = 0.009451028890907764
loss = 0.0003849492350127548
loss = 0.001358149223960936
loss = 0.00015814491780474782
loss = 0.07850174605846405
loss = 0.011878094635903835
loss = 0.0003881649172399193
loss = 0.00014031950559001416
loss = 0.00038300149026326835
loss = 0.019282296299934387
loss = 0.09034830331802368
loss = 0.0041710613295435905
loss = 0.17050395905971527
loss = 0.01813839189708233
loss = 0.0015614331932738423
loss = 0.511935293674469
loss = 0.005063171964138746
loss = 6.53935712762177e-05
loss = 0.24936020374298096
loss = 0.021595947444438934
loss = 0.0023338263854384422
loss = 0.0001901101932162419
loss = 0.37236908078193665
loss = 0.4172654449939728
loss = 0.0011143465526401997
loss = 0.00011140388232888654
loss = 0.00868773739784956
loss = 4.7700308641651645e-05
loss = 3.7948118460917613e-06
loss = 0.003698449581861496
loss = 0.21557171642780304
loss = 0.037846166640520096
loss = 5.504704677150585e-05
loss = 0.0010581404203549027
loss = 0.0014961963752284646
loss = 0.029669711366295815
loss = 0.08785632997751236
loss = 0.0009201755165122449
loss = 0.0
loss = 0.009823610074818134
loss = 0.00819403026252985
loss = 7.645531150046736e-05
loss = 0.023268474265933037
loss = 0.30208227038383484
loss = 0.29144904017448425
loss = 0.00677834777161479
loss = 0.002876484766602516
loss = 2.2649696802545805e-06
loss = 1.55524480342865
loss = 0.00339131080545485
loss = 1.0311493497283664e-05
loss = 0.00011645226186374202
loss = 0.0009550533723086119
loss = 0.5120398998260498
loss = 0.008634604513645172
loss = 0.0027899714186787605
loss = 1.7459774017333984
loss = 1.951022022694815e-05
loss = 0.0006543957279063761
loss = 0.07888089865446091
loss = 4.414515569806099e-05
loss = 0.09511997550725937
loss = 1.0569719052000437e-05
loss = 0.11013317853212357
loss = 0.009929792024195194
loss = 5.1550170610425994e-05
loss = 5.145827799424296e-06
loss = 0.0030505219474434853
loss = 0.0002789342252071947
loss = 0.005166922230273485
loss = 0.0028499269392341375
loss = 0.041064757853746414
loss = 4.569684222133219e-07
loss = 2.18550212593982e-07
loss = 0.0020713191479444504
loss = 0.30451902747154236
loss = 0.004745386075228453
loss = 0.15605956315994263
loss = 0.0021151395048946142
loss = 0.0002628207439556718
loss = 2.73772584478138e-05
loss = 0.9767155051231384
loss = 0.005934205371886492
loss = 0.00011521654232637957
loss = 0.0027388688176870346
loss = 0.0002845207054633647
loss = 3.6358476336317835e-06
loss = 0.2606297731399536
loss = 0.003923798445612192
loss = 1.4284822100307792e-05
loss = 0.00013531280274037272
loss = 0.05196816846728325
loss = 0.2811156213283539
loss = 0.009194575250148773
loss = 0.00036117006675340235
loss = 0.23679232597351074
loss = 0.00046490668319165707
loss = 0.11259853839874268
loss = 1.358075499534607
loss = 0.022801579907536507
loss = 0.02241482026875019
loss = 0.0010857968591153622
loss = 0.004841668996959925
loss = 3.7948109365970595e-06
loss = 0.00015705985424574465
loss = 0.030705267563462257
loss = 5.786681140307337e-05
loss = 0.00907321646809578
loss = 0.0137518011033535
loss = 0.0013473681174218655
loss = 0.006139421369880438
loss = 0.01600370556116104
loss = 8.80151037563337e-06
loss = 1.3510348253475968e-06
loss = 0.00016669895558152348
loss = 0.0004049583512824029
loss = 2.705893712118268e-05
loss = 0.00011350909335305914
loss = 3.774958940994111e-07
loss = 0.007950984872877598
loss = 0.9924337863922119
loss = 0.0004223079886287451
loss = 3.000073547809734e-06
loss = 3.973642392907095e-08
loss = 7.887499123171438e-06
loss = 0.00203080871142447
loss = 4.329033617977984e-05
loss = 1.972808786376845e-05
loss = 0.01597137749195099
loss = 1.9226337671279907
loss = 1.3072916772216558e-05
loss = 6.397443939931691e-06
loss = 5.086201326776063e-06
loss = 0.0027949176728725433
loss = 1.5815914869308472
loss = 0.10680302232503891
loss = 0.8203120827674866
loss = 0.0009318594820797443
loss = 0.00022037838061805815
loss = 0.1899101734161377
loss = 1.657868504524231
loss = 2.5828668981375813e-07
loss = 0.0025867712683975697
loss = 3.544363789842464e-05
loss = 0.00048049280303530395
loss = 0.12356048822402954
loss = 0.017019575461745262
loss = 0.1024232804775238
loss = 0.00015391994384117424
loss = 0.003895501373335719
loss = 1.096700088965008e-05
loss = 0.0005796959740109742
loss = 0.0005426917923614383
loss = 0.0011605402687564492
loss = 0.011345021426677704
loss = 0.0005791299627162516
loss = 0.05625350400805473
loss = 0.00013482087524607778
loss = 0.001896087545901537
loss = 0.24353520572185516
loss = 0.04260263219475746
loss = 0.0013035890879109502
loss = 1.5894563887286495e-07
loss = 0.0016537430929020047
loss = 0.030055128037929535
loss = 0.012293144129216671
loss = 7.212063792394474e-06
loss = 0.0007533530588261783
loss = 0.0011920048855245113
loss = 0.0001486876280978322
loss = 0.0005033843335695565
loss = 0.002589274663478136
loss = 7.361561438301578e-05
loss = 0.5316601395606995
loss = 0.03642341122031212
loss = 0.017340367659926414
loss = 0.21421374380588531
loss = 0.03119189478456974
loss = 2.7794189009000547e-05
loss = 0.005464752670377493
loss = 0.2747431993484497
loss = 0.00012791006884071976
loss = 0.0021773993503302336
loss = 0.008443085476756096
loss = 0.0005068972241133451
loss = 0.006237989757210016
loss = 0.7018704414367676
loss = 0.011203396134078503
loss = 8.373359014512971e-05
loss = 4.291482582630124e-06
loss = 0.00017891707830131054
loss = 1.6971784830093384
loss = 0.00015163714124355465
loss = 3.061597453779541e-05
loss = 0.0011542123975232244
loss = 0.013267777860164642
loss = 0.10793576389551163
loss = 1.301242470741272
loss = 0.004916524048894644
loss = 0.02718435414135456
loss = 0.6000311970710754
loss = 1.139104962348938
loss = 0.0002461700059939176
loss = 0.011484538204967976
loss = 0.02289532870054245
loss = 0.00027238507755100727
loss = 1.9470742245175643e-06
loss = 0.001014670473523438
loss = 0.04057662561535835
loss = 0.09740317612886429
loss = 0.4346768856048584
loss = 0.05142989382147789
loss = 0.01756168156862259
loss = 8.066309419518802e-06
loss = 0.008828207850456238
loss = 0.04805299639701843
loss = 0.0005017358344048262
loss = 0.0011049326276406646
loss = 0.0004180030955467373
loss = 0.3946821689605713
loss = 0.4440765380859375
loss = 0.0064363195560872555
loss = 2.8609986202354776e-06
loss = 0.009824506938457489
loss = 0.006983037572354078
loss = 0.052954331040382385
loss = 0.07825073599815369
loss = 0.00043095214641653
loss = 3.2186219414143125e-06
loss = 0.005998207721859217
loss = 0.016311226412653923
loss = 0.8993354439735413
loss = 0.0001027527978294529
loss = 9.300008969148621e-05
loss = 0.11572626978158951
loss = 0.0020285791251808405
loss = 0.2651689052581787
loss = 6.263330578804016e-05
loss = 1.0947082955681253e-05
loss = 3.9338897295237985e-06
loss = 0.0
loss = 1.8686408996582031
loss = 3.3775930319279723e-07
loss = 0.0005312199355103076
loss = 0.18670064210891724
loss = 6.0769485571654513e-05
loss = 0.002712684916332364
loss = 0.009836292825639248
loss = 0.0007504247478209436
loss = 0.002780014416202903
loss = 0.00011990255006821826
loss = 0.9588235020637512
loss = 0.0034159738570451736
loss = 0.0004193951899651438
loss = 0.12993396818637848
loss = 0.04082150757312775
loss = 0.004917618352919817
loss = 0.0494961179792881
loss = 0.00023485696874558926
loss = 0.3620758056640625
loss = 0.00014344723604153842
loss = 0.00032718220609240234
loss = 0.18577276170253754
loss = 0.037908464670181274
loss = 0.00014291224943008274
loss = 0.011631709523499012
loss = 4.152425844949903e-06
loss = 9.474765829509124e-05
loss = 0.05908563360571861
loss = 0.7400115132331848
loss = 0.016158415004611015
loss = 0.00015786585572641343
loss = 0.008259198628365993
loss = 0.035767823457717896
loss = 0.0009828603360801935
loss = 0.0011380919022485614
loss = 0.04983201250433922
loss = 0.002977960044518113
loss = 0.04236838221549988
loss = 0.0007830550894141197
loss = 0.7383150458335876
loss = 8.426068961853161e-05
loss = 5.326245445758104e-05
loss = 0.012523545883595943
loss = 0.00557960569858551
loss = 0.001946686883457005
loss = 0.010162551887333393
loss = 0.03755198419094086
loss = 0.00019657947996165603
loss = 0.12628379464149475
loss = 3.536521717251162e-06
loss = 0.0001640841510379687
loss = 0.0005034523201175034
loss = 1.6331245205947198e-05
loss = 0.19828717410564423
loss = 0.021340662613511086
loss = 2.3047014110488817e-06
loss = 0.00019685649021994323
loss = 0.0033709488343447447
loss = 0.045985784381628036
loss = 0.06764485687017441
loss = 1.1523544571900857e-06
loss = 0.0005487498128786683
loss = 0.4587728679180145
loss = 0.03775309771299362
loss = 0.21491025388240814
loss = 0.6620479822158813
loss = 0.009426897391676903
loss = 0.5140514373779297
loss = 7.74859188368282e-07
loss = 5.772704025730491e-05
loss = 0.003132038516923785
loss = 4.867667485086713e-06
loss = 0.000416822120314464
loss = 0.08278336375951767
loss = 2.0464174212975195e-06
loss = 9.374131332151592e-05
loss = 0.002757824258878827
loss = 0.43604159355163574
loss = 0.009956699796020985
loss = 0.019313126802444458
loss = 0.0006463883328251541
loss = 0.006948467344045639
loss = 0.010623793117702007
loss = 0.009556633420288563
loss = 0.0007071110303513706
loss = 0.28518542647361755
loss = 0.0026984820142388344
loss = 4.896796235698275e-05
loss = 0.0001178475795313716
loss = 0.00040379600250162184
loss = 1.102650730899768e-05
loss = 0.11999206990003586
loss = 5.026583949074848e-06
loss = 0.4192122220993042
loss = 0.5692853331565857
loss = 2.185418452427257e-05
loss = 0.19656617939472198
loss = 1.5366641283035278
loss = 0.0001622446725377813
loss = 0.6419036388397217
loss = 8.380351937375963e-05
loss = 0.000333831092575565
loss = 0.8732922673225403
loss = 0.16735447943210602
loss = 1.8278716424902086e-06
loss = 3.7269670428941026e-05
loss = 1.348828911781311
loss = 0.006665913853794336
loss = 0.014809995889663696
loss = 1.2206400632858276
loss = 5.761772285950428e-07
loss = 0.0005578674026764929
loss = 1.0808059414557647e-05
loss = 0.5088925957679749
loss = 0.0013924421509727836
loss = 0.19230739772319794
loss = 3.111171827185899e-05
loss = 8.615101251052693e-05
loss = 0.18927699327468872
loss = 0.06401713192462921
loss = 0.13668140769004822
loss = 0.0002974977542180568
loss = 0.00038083666004240513
loss = 8.046489710977767e-06
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.968" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31250&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="221.78380072785657" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1262a0610&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9717833333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="1007.0871659185926" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.968" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31250&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="221.78380072785657" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1262a0610&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9717833333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="1007.0871659185926" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
epoch 1 of 10: training_loss = 992.6620, training_accuracy = 0.9727, test_loss = 289.8473, test_accuracy = 0.9625
epoch 2 of 10: training_loss = 941.3189, training_accuracy = 0.9738, test_loss = 239.2692, test_accuracy = 0.9652
epoch 3 of 10: training_loss = 929.3977, training_accuracy = 0.9748, test_loss = 242.5372, test_accuracy = 0.9668
epoch 4 of 10: training_loss = 909.8816, training_accuracy = 0.9752, test_loss = 300.5773, test_accuracy = 0.9647
epoch 5 of 10: training_loss = 894.5522, training_accuracy = 0.9758, test_loss = 275.4314, test_accuracy = 0.9672
epoch 6 of 10: training_loss = 886.8376, training_accuracy = 0.9761, test_loss = 302.3428, test_accuracy = 0.9653
epoch 7 of 10: training_loss = 916.1014, training_accuracy = 0.9758, test_loss = 345.3399, test_accuracy = 0.9588
epoch 8 of 10: training_loss = 880.2075, training_accuracy = 0.9770, test_loss = 276.3994, test_accuracy = 0.9682
epoch 9 of 10: training_loss = 922.8217, training_accuracy = 0.9761, test_loss = 269.4280, test_accuracy = 0.9675
epoch 10 of 10: training_loss = 861.8362, training_accuracy = 0.9768, test_loss = 319.5486, test_accuracy = 0.9690
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="9" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.969" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31250&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="319.54857768638567" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1262a0610&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9768333333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="861.8361800547185" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="9" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.0001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.0001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.969" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c1fb50&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="319.54857768638567" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x12714d0d0&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9768333333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="861.8361800547185" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
epoch 1 of 30: training_loss = 4169.9043, training_accuracy = 0.8804, test_loss = 465.9116, test_accuracy = 0.9200
epoch 2 of 30: training_loss = 2180.0612, training_accuracy = 0.9357, test_loss = 302.5049, test_accuracy = 0.9479
epoch 3 of 30: training_loss = 1593.3946, training_accuracy = 0.9525, test_loss = 237.0273, test_accuracy = 0.9574
epoch 4 of 30: training_loss = 1262.9974, training_accuracy = 0.9621, test_loss = 184.0168, test_accuracy = 0.9643
epoch 5 of 30: training_loss = 1048.7970, training_accuracy = 0.9681, test_loss = 174.0410, test_accuracy = 0.9670
epoch 6 of 30: training_loss = 893.0911, training_accuracy = 0.9728, test_loss = 156.4148, test_accuracy = 0.9696
epoch 7 of 30: training_loss = 771.4336, training_accuracy = 0.9762, test_loss = 143.9270, test_accuracy = 0.9728
epoch 8 of 30: training_loss = 679.6036, training_accuracy = 0.9785, test_loss = 138.5616, test_accuracy = 0.9723
epoch 9 of 30: training_loss = 607.3600, training_accuracy = 0.9812, test_loss = 129.8913, test_accuracy = 0.9755
epoch 10 of 30: training_loss = 539.8175, training_accuracy = 0.9830, test_loss = 130.1541, test_accuracy = 0.9758
epoch 11 of 30: training_loss = 476.7642, training_accuracy = 0.9850, test_loss = 134.3858, test_accuracy = 0.9745
epoch 12 of 30: training_loss = 439.9832, training_accuracy = 0.9864, test_loss = 120.5108, test_accuracy = 0.9769
epoch 13 of 30: training_loss = 389.3329, training_accuracy = 0.9871, test_loss = 127.9708, test_accuracy = 0.9761
epoch 14 of 30: training_loss = 347.0043, training_accuracy = 0.9886, test_loss = 135.6495, test_accuracy = 0.9765
epoch 15 of 30: training_loss = 324.6224, training_accuracy = 0.9897, test_loss = 122.7319, test_accuracy = 0.9767
epoch 16 of 30: training_loss = 292.4515, training_accuracy = 0.9901, test_loss = 136.9033, test_accuracy = 0.9746
epoch 17 of 30: training_loss = 271.8879, training_accuracy = 0.9910, test_loss = 126.7766, test_accuracy = 0.9773
epoch 18 of 30: training_loss = 242.3240, training_accuracy = 0.9922, test_loss = 139.1980, test_accuracy = 0.9749
epoch 19 of 30: training_loss = 230.6792, training_accuracy = 0.9926, test_loss = 142.2222, test_accuracy = 0.9764
epoch 20 of 30: training_loss = 209.7111, training_accuracy = 0.9927, test_loss = 125.1445, test_accuracy = 0.9788
epoch 21 of 30: training_loss = 180.2585, training_accuracy = 0.9940, test_loss = 119.7948, test_accuracy = 0.9800
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="21" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.0001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.0001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.98" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c1fb50&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="119.79478976307644" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x12714d0d0&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.99395" />
<var name="training_loss" type="float" qualifier="builtins" value="180.2584555235279" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="21" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.98" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="119.79478976307644" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.99395" />
<var name="training_loss" type="float" qualifier="builtins" value="180.2584555235279" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
epoch 1 of 30: training_loss = 3069.5358, training_accuracy = 0.9043, test_loss = 280.0863, test_accuracy = 0.9463
epoch 2 of 30: training_loss = 1744.1568, training_accuracy = 0.9471, test_loss = 221.3735, test_accuracy = 0.9583
epoch 3 of 30: training_loss = 1448.1306, training_accuracy = 0.9557, test_loss = 222.6428, test_accuracy = 0.9583
epoch 4 of 30: training_loss = 1322.4673, training_accuracy = 0.9602, test_loss = 320.2245, test_accuracy = 0.9437
epoch 5 of 30: training_loss = 1174.1490, training_accuracy = 0.9642, test_loss = 224.6675, test_accuracy = 0.9641
epoch 6 of 30: training_loss = 1129.3574, training_accuracy = 0.9658, test_loss = 232.4387, test_accuracy = 0.9619
epoch 7 of 30: training_loss = 1052.9097, training_accuracy = 0.9689, test_loss = 285.7811, test_accuracy = 0.9536
epoch 8 of 30: training_loss = 1036.2252, training_accuracy = 0.9699, test_loss = 256.6864, test_accuracy = 0.9612
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="8" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9612" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="256.6863803491975" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9699" />
<var name="training_loss" type="float" qualifier="builtins" value="1036.2251639158162" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
epoch 1 of 30: training_loss = 1000.1048, training_accuracy = 0.9711, test_loss = 261.3892, test_accuracy = 0.9661
epoch 2 of 30: training_loss = 931.7344, training_accuracy = 0.9736, test_loss = 262.0217, test_accuracy = 0.9628
epoch 3 of 30: training_loss = 955.9140, training_accuracy = 0.9726, test_loss = 319.6526, test_accuracy = 0.9578
epoch 4 of 30: training_loss = 919.7248, training_accuracy = 0.9738, test_loss = 260.0467, test_accuracy = 0.9653
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="4" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9653" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="260.0467114993509" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9737833333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="919.7248096111103" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="0" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9653" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="260.0467114993509" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9737833333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="919.7248096111103" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
epoch 1 of 30: training_loss = 891.2577, training_accuracy = 0.9748, test_loss = 287.5357, test_accuracy = 0.9674
epoch 2 of 30: training_loss = 855.1894, training_accuracy = 0.9765, test_loss = 402.6232, test_accuracy = 0.9563
epoch 3 of 30: training_loss = 890.5682, training_accuracy = 0.9755, test_loss = 325.6192, test_accuracy = 0.9592
epoch 4 of 30: training_loss = 887.8625, training_accuracy = 0.9769, test_loss = 295.9535, test_accuracy = 0.9675
epoch 5 of 30: training_loss = 864.7137, training_accuracy = 0.9769, test_loss = 296.1682, test_accuracy = 0.9654
epoch 6 of 30: training_loss = 879.8001, training_accuracy = 0.9764, test_loss = 360.2599, test_accuracy = 0.9564
epoch 7 of 30: training_loss = 868.7477, training_accuracy = 0.9772, test_loss = 345.7147, test_accuracy = 0.9596
epoch 8 of 30: training_loss = 882.1926, training_accuracy = 0.9772, test_loss = 280.1760, test_accuracy = 0.9709
epoch 9 of 30: training_loss = 924.0175, training_accuracy = 0.9765, test_loss = 418.3430, test_accuracy = 0.9604
epoch 10 of 30: training_loss = 913.0257, training_accuracy = 0.9772, test_loss = 399.9818, test_accuracy = 0.9667
epoch 11 of 30: training_loss = 872.0682, training_accuracy = 0.9766, test_loss = 312.8230, test_accuracy = 0.9683
epoch 12 of 30: training_loss = 844.5870, training_accuracy = 0.9778, test_loss = 372.3970, test_accuracy = 0.9571
epoch 13 of 30: training_loss = 898.5133, training_accuracy = 0.9774, test_loss = 453.5815, test_accuracy = 0.9642
epoch 14 of 30: training_loss = 878.7675, training_accuracy = 0.9776, test_loss = 313.7045, test_accuracy = 0.9638
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="14" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9638" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="313.70452806335743" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9776166666666667" />
<var name="training_loss" type="float" qualifier="builtins" value="878.7674642182853" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="14" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9638" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="313.70452806335743" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9776166666666667" />
<var name="training_loss" type="float" qualifier="builtins" value="878.7674642182853" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
epoch 1 of 30: training_loss = 927.8648, training_accuracy = 0.9775, test_loss = 312.7312, test_accuracy = 0.9702
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="1" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9702" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="312.73121757189733" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9774666666666667" />
<var name="training_loss" type="float" qualifier="builtins" value="927.8648430596875" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
epoch 1 of 30: training_loss = 905.5375, training_accuracy = 0.9775, test_loss = 407.3147, test_accuracy = 0.9606
epoch 2 of 30: training_loss = 924.8586, training_accuracy = 0.9774, test_loss = 341.2776, test_accuracy = 0.9676
epoch 3 of 30: training_loss = 894.7821, training_accuracy = 0.9777, test_loss = 439.3148, test_accuracy = 0.9678
epoch 4 of 30: training_loss = 879.2174, training_accuracy = 0.9787, test_loss = 419.7928, test_accuracy = 0.9678
epoch 5 of 30: training_loss = 903.3500, training_accuracy = 0.9784, test_loss = 377.2813, test_accuracy = 0.9654
epoch 6 of 30: training_loss = 880.2251, training_accuracy = 0.9785, test_loss = 394.8594, test_accuracy = 0.9682
epoch 7 of 30: training_loss = 871.7683, training_accuracy = 0.9789, test_loss = 429.1205, test_accuracy = 0.9681
epoch 8 of 30: training_loss = 943.5691, training_accuracy = 0.9780, test_loss = 352.0782, test_accuracy = 0.9683
epoch 9 of 30: training_loss = 891.2567, training_accuracy = 0.9783, test_loss = 461.9374, test_accuracy = 0.9629
epoch 10 of 30: training_loss = 861.8392, training_accuracy = 0.9788, test_loss = 352.8065, test_accuracy = 0.9690
epoch 11 of 30: training_loss = 916.4445, training_accuracy = 0.9785, test_loss = 405.9475, test_accuracy = 0.9693
epoch 12 of 30: training_loss = 893.4083, training_accuracy = 0.9787, test_loss = 500.4407, test_accuracy = 0.9649
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="12" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9649" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="500.4406801519303" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9787333333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="893.4083315522987" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="12" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9649" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="500.4406801519303" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9787333333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="893.4083315522987" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="12" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9649" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="500.4406801519303" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9787333333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="893.4083315522987" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="12" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9649" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="500.4406801519303" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9787333333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="893.4083315522987" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="12" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9649" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="500.4406801519303" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9787333333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="893.4083315522987" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="12" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9649" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x127066580&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="500.4406801519303" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x125c31280&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9787333333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="893.4083315522987" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="12" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9649" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1443edf40&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="500.4406801519303" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1443edb80&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9787333333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="893.4083315522987" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
<xml><var name="_dummy_ipython_val"  />
<var name="_dummy_special_var"  />
<var name="MNIST_model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="batch_size" type="int" qualifier="builtins" value="6" />
<var name="criterion" type="CrossEntropyLoss" qualifier="torch.nn.modules.loss" value="CrossEntropyLoss%28%29" isContainer="True" />
<var name="epoch" type="int" qualifier="builtins" value="12" />
<var name="epochs" type="int" qualifier="builtins" value="30" />
<var name="hidden_size" type="int" qualifier="builtins" value="128" />
<var name="learning_rate" type="float" qualifier="builtins" value="0.001" />
<var name="model" type="MLP" qualifier="__main__" value="MLP%28%0A  %28fc1%29%3A Linear%28in_features=784%2C out_features=128%2C bias=True%29%0A  %28relu1%29%3A ReLU%28%29%0A  %28fc2%29%3A Linear%28in_features=128%2C out_features=128%2C bias=True%29%0A  %28relu2%29%3A ReLU%28%29%0A  %28fc3%29%3A Linear%28in_features=128%2C out_features=10%2C bias=True%29%0A%29" isContainer="True" />
<var name="optimizer" type="Adam" qualifier="torch.optim.adam" value="Adam %28%0AParameter Group 0%0A    amsgrad%3A False%0A    betas%3A %280.9%2C 0.999%29%0A    capturable%3A False%0A    differentiable%3A False%0A    eps%3A 1e-08%0A    foreach%3A None%0A    fused%3A None%0A    lr%3A 0.001%0A    maximize%3A False%0A    weight_decay%3A 0%0A%29" isContainer="True" />
<var name="test_acc" type="float" qualifier="builtins" value="0.9649" />
<var name="test_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 10000%0A    Root location%3A ./data%0A    Split%3A Test%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="10000" />
<var name="test_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1443edf40&gt;" isContainer="True" shape="1667" />
<var name="test_loss" type="float" qualifier="builtins" value="500.4406801519303" />
<var name="train_dataset" type="MNIST" qualifier="torchvision.datasets.mnist" value="Dataset MNIST%0A    Number of datapoints%3A 60000%0A    Root location%3A ./data%0A    Split%3A Train%0A    StandardTransform%0ATransform%3A Compose%28%0A               ToTensor%28%29%0A               Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A           %29" isContainer="True" shape="60000" />
<var name="train_loader" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1443edb80&gt;" isContainer="True" shape="10000" />
<var name="training_acc" type="float" qualifier="builtins" value="0.9787333333333333" />
<var name="training_loss" type="float" qualifier="builtins" value="893.4083315522987" />
<var name="transform" type="Compose" qualifier="torchvision.transforms.transforms" value="Compose%28%0A    ToTensor%28%29%0A    Normalize%28mean=%280.5%2C%29%2C std=%280.5%2C%29%29%0A%29" isContainer="True" />
</xml>
[34m[1mwandb[0m: [33mWARNING[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.
epoch 1 of 30: training_loss = 3059.8235, training_accuracy = 0.9030, test_loss = 370.8893, test_accuracy = 0.9309
epoch 2 of 30: training_loss = 1723.9160, training_accuracy = 0.9471, test_loss = 253.7930, test_accuracy = 0.9526
epoch 3 of 30: training_loss = 1463.2681, training_accuracy = 0.9551, test_loss = 238.5171, test_accuracy = 0.9586
epoch 4 of 30: training_loss = 1314.7411, training_accuracy = 0.9606, test_loss = 216.8818, test_accuracy = 0.9577
epoch 5 of 30: training_loss = 1197.0129, training_accuracy = 0.9652, test_loss = 207.9014, test_accuracy = 0.9617
epoch 6 of 30: training_loss = 1099.7264, training_accuracy = 0.9669, test_loss = 229.7003, test_accuracy = 0.9629
epoch 7 of 30: training_loss = 1060.4155, training_accuracy = 0.9681, test_loss = 271.1025, test_accuracy = 0.9543
epoch 8 of 30: training_loss = 1021.5709, training_accuracy = 0.9694, test_loss = 296.2312, test_accuracy = 0.9554
epoch 9 of 30: training_loss = 972.5415, training_accuracy = 0.9718, test_loss = 215.2033, test_accuracy = 0.9675
epoch 10 of 30: training_loss = 954.8463, training_accuracy = 0.9719, test_loss = 301.4980, test_accuracy = 0.9597
epoch 11 of 30: training_loss = 945.4131, training_accuracy = 0.9731, test_loss = 231.4581, test_accuracy = 0.9671
epoch 12 of 30: training_loss = 957.5797, training_accuracy = 0.9731, test_loss = 306.9817, test_accuracy = 0.9570
epoch 13 of 30: training_loss = 918.8331, training_accuracy = 0.9742, test_loss = 247.0153, test_accuracy = 0.9670
epoch 14 of 30: training_loss = 918.5044, training_accuracy = 0.9748, test_loss = 277.5151, test_accuracy = 0.9640
epoch 15 of 30: training_loss = 914.2522, training_accuracy = 0.9742, test_loss = 229.2443, test_accuracy = 0.9692
epoch 16 of 30: training_loss = 891.6821, training_accuracy = 0.9758, test_loss = 310.8365, test_accuracy = 0.9627
epoch 17 of 30: training_loss = 873.6430, training_accuracy = 0.9761, test_loss = 266.5443, test_accuracy = 0.9666
epoch 18 of 30: training_loss = 914.5414, training_accuracy = 0.9758, test_loss = 237.7558, test_accuracy = 0.9693
epoch 19 of 30: training_loss = 873.6945, training_accuracy = 0.9769, test_loss = 264.0215, test_accuracy = 0.9683
epoch 20 of 30: training_loss = 857.8611, training_accuracy = 0.9772, test_loss = 290.9528, test_accuracy = 0.9688
epoch 21 of 30: training_loss = 909.0358, training_accuracy = 0.9766, test_loss = 293.7216, test_accuracy = 0.9671
epoch 22 of 30: training_loss = 905.8411, training_accuracy = 0.9768, test_loss = 372.7746, test_accuracy = 0.9582
epoch 23 of 30: training_loss = 917.8636, training_accuracy = 0.9768, test_loss = 364.8291, test_accuracy = 0.9656
epoch 24 of 30: training_loss = 921.6534, training_accuracy = 0.9774, test_loss = 307.3938, test_accuracy = 0.9668
